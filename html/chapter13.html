<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第13章：注意力机制优化</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">边缘侧大语言模型推理加速：从算法到系统</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：边缘推理的挑战与机遇</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：性能分析与Roofline模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：小语言模型(SLM)概览</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：后训练量化（PTQ）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：Hessian引导的量化方法</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：旋转量化与极低比特量化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：量化友好的模型设计</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：量化工具链</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：模型剪枝</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：稀疏化与参数共享</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：动态网络架构</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：知识蒸馏</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：注意力机制优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：KV Cache管理与压缩</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：解码加速技术</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：首Token延迟(TTFT)优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：内存管理与Offloading</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：边缘推理框架</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：深度学习编译器</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：硬件特定优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：跨平台部署实践</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：视觉编码器优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter23.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第23章：多模态融合与平衡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter24.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第24章：实时语音场景优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter25.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第25章：神经架构搜索（NAS）</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter26.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第26章：未来技术展望</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">项目说明</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="13">第13章：注意力机制优化</h1>
<p>注意力机制是Transformer架构的核心组件，但也是计算和内存的主要瓶颈。本章深入探讨注意力机制的优化技术，从算法层面的改进到系统层面的实现优化，涵盖Flash Attention、多查询注意力、稀疏注意力模式以及线性注意力等前沿技术。这些优化对于在边缘设备上高效部署大语言模型至关重要。</p>
<h2 id="131-flash-attention">13.1 Flash Attention原理与实现</h2>
<h3 id="1311">13.1.1 标准注意力的计算瓶颈</h3>
<p>标准的缩放点积注意力（Scaled Dot-Product Attention）计算公式为：</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
其中，$Q, K, V \in \mathbb{R}^{N \times d}$，$N$是序列长度，$d$是特征维度。</p>
<p><strong>内存复杂度分析</strong>：</p>
<ul>
<li>存储注意力矩阵 $S = QK^T$ 需要 $O(N^2)$ 内存</li>
<li>对于长序列（如 $N = 2048$），这会产生 4M 个元素的中间矩阵</li>
<li>在多头注意力中，这个开销会乘以头数</li>
<li>以FP16存储，仅注意力矩阵就需要 8MB 内存</li>
</ul>
<p><strong>计算复杂度细分</strong>：</p>
<ol>
<li>矩阵乘法 $S = QK^T$：$2N^2d$ FLOPs</li>
<li>Softmax计算：约 $5N^2$ FLOPs（exp、sum、div操作）</li>
<li>输出计算 $O = PV$：$2N^2d$ FLOPs</li>
<li>总计：约 $4N^2d + 5N^2$ FLOPs</li>
</ol>
<p><strong>带宽瓶颈</strong>：
标准实现需要多次读写HBM（高带宽内存）：</p>
<ol>
<li>加载 $Q, K$ 计算 $S = QK^T$：读取 $2Nd$ 个元素</li>
<li>写回 $S$ 到HBM：写入 $N^2$ 个元素</li>
<li>读取 $S$ 计算 softmax：读取 $N^2$ 个元素</li>
<li>写回 $P = \text{softmax}(S)$：写入 $N^2$ 个元素</li>
<li>读取 $P, V$ 计算最终输出：读取 $N^2 + Nd$ 个元素</li>
</ol>
<p>总的HBM访问量约为 $O(N^2d + Nd^2)$。</p>
<p><strong>硬件特性考虑</strong>：
现代GPU的内存层次结构：</p>
<ul>
<li>SRAM（共享内存）：~100KB，带宽 ~19TB/s</li>
<li>HBM（全局内存）：~40GB，带宽 ~1.5TB/s</li>
<li>带宽差距：约13倍</li>
</ul>
<p>这意味着如果能将计算保持在SRAM中，理论上可获得超过10倍的性能提升。</p>
<h3 id="1312-flash-attention">13.1.2 Flash Attention的核心思想</h3>
<p>Flash Attention通过<strong>平铺（tiling）</strong>和<strong>重计算（recomputation）</strong>来优化内存访问：</p>
<ol>
<li><strong>分块计算</strong>：将输入分成小块，使其能够完全放入SRAM（片上内存）</li>
<li><strong>融合操作</strong>：在SRAM中完成矩阵乘法和softmax，避免中间结果写回HBM</li>
<li><strong>增量softmax</strong>：使用在线算法计算softmax，无需存储完整的注意力矩阵</li>
</ol>
<p><strong>关键洞察</strong>：</p>
<ul>
<li>标准实现的瓶颈不是计算，而是内存带宽</li>
<li>通过减少HBM访问次数，即使增加一些重复计算也能获得净收益</li>
<li>利用softmax的数学性质，可以分块计算并正确合并结果</li>
</ul>
<p><strong>算法创新点</strong>：</p>
<ol>
<li><strong>在线softmax算法</strong>：无需两遍扫描即可计算softmax</li>
<li><strong>平铺策略</strong>：根据SRAM大小优化块尺寸</li>
<li><strong>数值稳定性</strong>：通过动态调整数值范围避免溢出</li>
</ol>
<h3 id="1313">13.1.3 分块算法详解</h3>
<p>设块大小为 $B_r \times B_c$，将 $Q$ 分成 $T_r = \lceil N/B_r \rceil$ 块，$K, V$ 分成 $T_c = \lceil N/B_c \rceil$ 块。</p>
<p><strong>块划分策略</strong>：</p>
<ul>
<li>$Q$ 矩阵：$Q = [Q_1; Q_2; ...; Q_{T_r}]$，每个 $Q_i \in \mathbb{R}^{B_r \times d}$</li>
<li>$K$ 矩阵：$K = [K_1; K_2; ...; K_{T_c}]$，每个 $K_j \in \mathbb{R}^{B_c \times d}$</li>
<li>$V$ 矩阵：$V = [V_1; V_2; ...; V_{T_c}]$，每个 $V_j \in \mathbb{R}^{B_c \times d}$</li>
</ul>
<p><strong>外层循环</strong>（对每个 $Q$ 块）：</p>
<pre class="codehilite"><code>对于 i = 1 到 T_r:
    加载 Q_i 到SRAM
    初始化: O_i = 0, l_i = 0, m_i = -∞

    内层循环（对每个 K,V 块）：
    对于 j = 1 到 T_c:
        加载 K_j, V_j 到SRAM
        计算 S_{ij} = Q_i K_j^T / √d_k  # 大小: B_r × B_c

        # 增量softmax更新
        m_{ij} = rowmax(S_{ij})         # 每行的最大值
        P_{ij} = exp(S_{ij} - m_{ij})   # 数值稳定的exp
        l_{ij} = rowsum(P_{ij})          # 每行的和

        # 更新运行统计
        m_i^{new} = max(m_i, m_{ij})
        l_i^{new} = exp(m_i - m_i^{new}) * l_i + exp(m_{ij} - m_i^{new}) * l_{ij}

        # 更新输出
        O_i = diag(exp(m_i - m_i^{new}))^{-1} * O_i + 
              diag(exp(m_{ij} - m_i^{new}))^{-1} * P_{ij} * V_j

        m_i = m_i^{new}, l_i = l_i^{new}

    写回 O_i 到HBM
</code></pre>

<p><strong>算法正确性保证</strong>：
该算法正确实现了标准注意力的原因在于：</p>
<ol>
<li>每个输出块 $O_i$ 独立计算，对应于 $Q_i$ 与所有 $K, V$ 的注意力</li>
<li>增量更新确保了跨块的softmax归一化正确性</li>
<li>数值稳定性通过动态调整最大值 $m_i$ 来保证</li>
</ol>
<h3 id="1314-softmax">13.1.4 增量Softmax的数学推导</h3>
<p>关键在于正确地合并部分softmax结果。设已处理前 $j-1$ 块，现处理第 $j$ 块：</p>
<p><strong>推导基础</strong>：
完整的softmax计算为：
$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$
为了数值稳定性，通常减去最大值：
$$\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}$$
<strong>增量更新推导</strong>：
设已处理的部分结果为：</p>
<ul>
<li>$m_i^{(j-1)}$：前 $j-1$ 块的行最大值</li>
<li>$l_i^{(j-1)}$：前 $j-1$ 块的归一化因子（未归一化的softmax分母）</li>
<li>$O_i^{(j-1)}$：前 $j-1$ 块的加权输出和</li>
</ul>
<p>处理第 $j$ 块时：</p>
<p><strong>最大值更新</strong>：
$$m_i^{(j)} = \max(m_i^{(j-1)}, m_{ij})$$
<strong>归一化因子更新</strong>：
需要将旧的归一化因子调整到新的数值范围：
$$l_i^{(j)} = e^{m_i^{(j-1)} - m_i^{(j)}} l_i^{(j-1)} + \sum_k e^{S_{ijk} - m_i^{(j)}}$$
这里：</p>
<ul>
<li>$e^{m_i^{(j-1)} - m_i^{(j)}}$ 是缩放因子，调整旧的累积和</li>
<li>$\sum_k e^{S_{ijk} - m_i^{(j)}}$ 是新块的贡献</li>
</ul>
<p><strong>输出更新</strong>：
$$O_i^{(j)} = \frac{e^{m_i^{(j-1)} - m_i^{(j)}} l_i^{(j-1)}}{l_i^{(j)}} O_i^{(j-1)} + \frac{1}{l_i^{(j)}} \sum_k e^{S_{ijk} - m_i^{(j)}} V_{jk}$$
证明这等价于完整计算：
$$O_i = \sum_{j,k} \frac{e^{S_{ijk} - \max_l S_{il}}}{\sum_{j',k'} e^{S_{ij'k'} - \max_l S_{il}}} V_{jk}$$</p>
<h3 id="1315">13.1.5 内存和计算复杂度分析</h3>
<p><strong>SRAM使用量详细分析</strong>：</p>
<ul>
<li>存储一个 $Q$ 块：$B_r \times d$ 个元素</li>
<li>存储一个 $K, V$ 块：$2 \times B_c \times d$ 个元素</li>
<li>存储中间结果 $S_{ij}$：$B_r \times B_c$ 个元素</li>
<li>存储统计量 $m_i, l_i$：$2 \times B_r$ 个元素</li>
<li>存储输出块 $O_i$：$B_r \times d$ 个元素</li>
<li>总计：$O((B_r + B_c) \times d + B_r \times B_c)$</li>
</ul>
<p><strong>HBM访问量对比</strong>：</p>
<p>标准注意力：</p>
<ul>
<li>读取 $Q, K, V$：$3Nd$ 次</li>
<li>写入/读取 $S$：$2N^2$ 次</li>
<li>写入/读取 $P$：$2N^2$ 次</li>
<li>总计：$O(N^2 + Nd)$ 次HBM访问</li>
</ul>
<p>Flash Attention：</p>
<ul>
<li>读取 $Q$：每块读一次，总计 $Nd$ 次</li>
<li>读取 $K, V$：每个 $Q$ 块都要读所有 $K, V$，总计 $2T_r \times Nd = 2Nd$ 次</li>
<li>写回输出：$Nd$ 次</li>
<li>总计：$O(Nd)$ 次HBM访问</li>
</ul>
<p><strong>带宽需求降低</strong>：</p>
<ul>
<li>标准实现：需要 $O(N^2)$ 的带宽</li>
<li>Flash Attention：只需要 $O(Nd)$ 的带宽</li>
<li>改善因子：$O(N/d)$，对于长序列效果显著</li>
</ul>
<p><strong>计算复杂度分析</strong>：</p>
<ul>
<li>每个 $(i,j)$ 块对：计算 $S_{ij} = Q_i K_j^T$ 需要 $2B_r B_c d$ FLOPs</li>
<li>总块对数：$T_r \times T_c = N^2/(B_r B_c)$</li>
<li>总FLOPs：$2N^2d$，与标准实现相同</li>
<li>额外的softmax更新计算：$O(N^2)$，相对较小</li>
</ul>
<h3 id="1316-flash-attention-v2">13.1.6 Flash Attention v2的改进</h3>
<p>Flash Attention v2主要优化了算法的并行性和硬件利用率：</p>
<ol>
<li>
<p><strong>改进的工作分配</strong>：
   - v1: 外层循环并行化（每个线程块处理一个 $Q$ 块）
   - v2: 更细粒度的并行化，减少线程块间的负载不均衡
   - 使用2D并行化：同时在 $Q$ 和 $KV$ 维度上分配工作</p>
</li>
<li>
<p><strong>减少非矩阵乘法操作</strong>：
   - 优化了softmax的实现，减少了标量操作
   - 使用向量化指令处理统计量更新
   - 减少了warp级别的同步开销</p>
</li>
<li>
<p><strong>更好的序列并行支持</strong>：
   - 支持跨GPU的序列维度切分
   - 优化了长序列的处理
   - 引入了因果掩码的高效处理</p>
</li>
</ol>
<p><strong>性能提升的关键技术</strong>：</p>
<p><strong>1. Warp级别的优化</strong>：
- 利用warp shuffle指令减少共享内存访问
- 每个warp处理一个完整的行，避免跨warp通信
- Warp-level reduction：利用__shfl_down_sync等指令在warp内高效求和</p>
<p><strong>2. 数据布局优化</strong>：
- 使用转置的 $K$ 存储，提高内存合并访问
- 优化了不同头之间的数据布局
- Bank conflict避免：通过padding确保不同线程访问不同的bank</p>
<p><strong>3. 混合精度策略</strong>：
- 累加器使用FP32保证精度
- 输入输出使用FP16/BF16节省带宽
- 动态范围调整避免数值溢出</p>
<p><strong>4. 算法级优化</strong>：</p>
<p><strong>分割策略改进</strong>：
Flash v2使用了更智能的分割策略，考虑了硬件的并行度：</p>
<ul>
<li>SM (Streaming Multiprocessor) 数量：根据GPU的SM数量调整并行块数</li>
<li>Warp调度：每个线程块的warp数优化为2的幂次，提高占用率</li>
<li>寄存器压力：通过减少中间变量，提高每个SM能同时执行的线程块数</li>
</ul>
<p><strong>数学优化</strong>：
在线softmax算法的进一步优化，减少数值计算：
$$m_i^{new} = \max(m_i^{old}, m_{ij})$$
$$l_i^{new} = e^{m_i^{old} - m_i^{new}} \cdot l_i^{old} + e^{m_{ij} - m_i^{new}} \cdot l_{ij}$$
Flash v2通过预计算 $e^{-m_i^{new}}$ 并复用，减少指数运算次数。</p>
<p><strong>5. 因果掩码的优化处理</strong>：</p>
<p>对于自回归生成，Flash v2引入了高效的因果掩码处理：</p>
<ul>
<li>隐式掩码：不显式存储掩码矩阵，而是在计算时动态判断</li>
<li>提前退出：当 $j &gt; i$ 时，直接跳过计算</li>
<li>负载均衡：通过对角线分块，确保每个线程块的工作量相近</li>
</ul>
<p><strong>6. 向后传播的优化</strong>：</p>
<p>Flash v2不仅优化了前向传播，还显著改进了反向传播：</p>
<ul>
<li>重计算策略：只存储必要的中间结果（如logsumexp）</li>
<li>原子操作优化：使用更高效的原子加操作累积梯度</li>
<li>梯度累积：批量更新减少内存事务</li>
</ul>
<p><strong>性能对比</strong>（A100 GPU）：
| 序列长度 | Flash v1 | Flash v2 | 提升比例 |</p>
<table>
<thead>
<tr>
<th>序列长度</th>
<th>Flash v1</th>
<th>Flash v2</th>
<th>提升比例</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1.5ms</td>
<td>0.9ms</td>
<td>1.67×</td>
</tr>
<tr>
<td>2048</td>
<td>23ms</td>
<td>12ms</td>
<td>1.92×</td>
</tr>
<tr>
<td>8192</td>
<td>370ms</td>
<td>180ms</td>
<td>2.06×</td>
</tr>
<tr>
<td>16384</td>
<td>1480ms</td>
<td>680ms</td>
<td>2.18×</td>
</tr>
</tbody>
</table>
<p><strong>内存带宽利用率对比</strong>：
| 方法 | 理论带宽利用率 | 实际测量（A100）|</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>理论带宽利用率</th>
<th>实际测量（A100）</th>
</tr>
</thead>
<tbody>
<tr>
<td>标准注意力</td>
<td>~15%</td>
<td>12-18%</td>
</tr>
<tr>
<td>Flash v1</td>
<td>~45%</td>
<td>38-42%</td>
</tr>
<tr>
<td>Flash v2</td>
<td>~72%</td>
<td>65-70%</td>
</tr>
</tbody>
</table>
<p><strong>不同精度下的性能</strong>（序列长度4096）：
| 精度配置 | Flash v1 | Flash v2 | 相对提升 |</p>
<table>
<thead>
<tr>
<th>精度配置</th>
<th>Flash v1</th>
<th>Flash v2</th>
<th>相对提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>95ms</td>
<td>52ms</td>
<td>1.83×</td>
</tr>
<tr>
<td>FP16</td>
<td>46ms</td>
<td>24ms</td>
<td>1.92×</td>
</tr>
<tr>
<td>BF16</td>
<td>45ms</td>
<td>23ms</td>
<td>1.96×</td>
</tr>
<tr>
<td>INT8*</td>
<td>-</td>
<td>18ms</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>*INT8支持仅在Flash v2.5+版本</p>
<h3 id="1317">13.1.7 边缘设备上的适配考虑</h3>
<p>在边缘设备上实现Flash Attention需要考虑硬件特性、内存限制和能效约束：</p>
<h4 id="13171">13.1.7.1 硬件特性分析</h4>
<p><strong>1. 有限的片上内存</strong>：</p>
<p>不同边缘硬件的内存层次对比：</p>
<p>| 硬件平台 | L1缓存 | L2缓存 | 共享内存 | 建议块大小 |</p>
<table>
<thead>
<tr>
<th>硬件平台</th>
<th>L1缓存</th>
<th>L2缓存</th>
<th>共享内存</th>
<th>建议块大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>ARM Cortex-A78</td>
<td>64KB</td>
<td>512KB</td>
<td>-</td>
<td>B=32-48</td>
</tr>
<tr>
<td>Apple M2</td>
<td>128KB</td>
<td>4MB</td>
<td>32KB</td>
<td>B=64-96</td>
</tr>
<tr>
<td>Snapdragon 8Gen2</td>
<td>64KB</td>
<td>1MB</td>
<td>16KB (GPU)</td>
<td>B=32</td>
</tr>
<tr>
<td>Mali-G710</td>
<td>-</td>
<td>-</td>
<td>16KB</td>
<td>B=24-32</td>
</tr>
<tr>
<td>Adreno 740</td>
<td>-</td>
<td>-</td>
<td>32KB</td>
<td>B=48</td>
</tr>
</tbody>
</table>
<p><strong>块大小选择的数学分析</strong>：</p>
<p>给定片上内存大小 $M_{on-chip}$，需要存储：</p>
<ul>
<li>$Q$ 块：$B_r \times d \times \text{sizeof}(\text{dtype})$</li>
<li>$K, V$ 块：$2 \times B_c \times d \times \text{sizeof}(\text{dtype})$  </li>
<li>中间结果 $S_{ij}$：$B_r \times B_c \times \text{sizeof}(\text{dtype})$</li>
<li>统计量：$2 \times B_r \times \text{sizeof}(\text{float32})$</li>
</ul>
<p>约束条件：
$$B_r d + 2B_c d + B_r B_c + 8B_r \leq \frac{M_{on-chip}}{\text{sizeof}(\text{dtype})}$$</p>
<p><strong>2. 向量化指令集差异</strong>：</p>
<p>| 指令集 | 向量宽度 | 特点 | Flash Attention优化策略 |</p>
<table>
<thead>
<tr>
<th>指令集</th>
<th>向量宽度</th>
<th>特点</th>
<th>Flash Attention优化策略</th>
</tr>
</thead>
<tbody>
<tr>
<td>NEON</td>
<td>128-bit</td>
<td>ARM标准SIMD</td>
<td>4个FP32或8个FP16并行</td>
</tr>
<tr>
<td>SVE/SVE2</td>
<td>128-2048bit</td>
<td>可变长度向量</td>
<td>动态适配向量长度</td>
</tr>
<tr>
<td>AMX</td>
<td>512-bit</td>
<td>Apple矩阵扩展</td>
<td>利用矩阵乘法单元</td>
</tr>
<tr>
<td>HVX</td>
<td>1024-bit</td>
<td>Hexagon向量扩展</td>
<td>超宽SIMD并行</td>
</tr>
</tbody>
</table>
<p><strong>3. 内存带宽限制</strong>：</p>
<p>边缘设备内存带宽远低于数据中心GPU：</p>
<ul>
<li>移动LPDDR5：51.2 GB/s</li>
<li>M2 Pro：200 GB/s  </li>
<li>A100 HBM：2 TB/s</li>
</ul>
<p>这使得Flash Attention的内存优化在边缘设备上更加重要。</p>
<h4 id="13172">13.1.7.2 算法适配策略</h4>
<p><strong>1. 多级分块策略</strong>：</p>
<p>针对边缘设备的多级缓存，采用嵌套分块：</p>
<ul>
<li>L2级分块：大小为 $B_{L2} = \sqrt{L2_size / (3 \times \text{sizeof}(\text{dtype}))}$</li>
<li>L1级分块：大小为 $B_{L1} = \sqrt{L1_size / (3 \times \text{sizeof}(\text{dtype}))}$</li>
</ul>
<p>计算流程：</p>
<pre class="codehilite"><code>for each L2_block in range(0, N, B_L2):
    # 预取L2块到L2缓存
    prefetch_L2_block()

    for each L1_block in range(L2_block, L2_block+B_L2, B_L1):
        # 在L1缓存中计算
        compute_attention_block()
</code></pre>

<p><strong>2. 混合精度计算策略</strong>：</p>
<p>针对不同计算阶段使用不同精度：</p>
<p>| 计算阶段 | 推荐精度 | 原因 |</p>
<table>
<thead>
<tr>
<th>计算阶段</th>
<th>推荐精度</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>$S = QK^T$</td>
<td>INT8/FP16</td>
<td>矩阵乘法，精度要求适中</td>
</tr>
<tr>
<td>$m_i, l_i$ 更新</td>
<td>FP32</td>
<td>累积误差敏感</td>
</tr>
<tr>
<td>$\exp(S - m)$</td>
<td>FP16 + LUT</td>
<td>指数运算开销大</td>
</tr>
<tr>
<td>最终输出</td>
<td>INT8/FP16</td>
<td>匹配模型整体精度</td>
</tr>
</tbody>
</table>
<p><strong>3. 指数运算优化</strong>：</p>
<p>边缘设备上指数运算开销巨大，优化方案：</p>
<p><strong>方案1：分段线性近似</strong>
$$\exp(x) \approx \begin{cases}
0 &amp; x &lt; -5 \\
a_i x + b_i &amp; x \in [x_i, x_{i+1}] \\
\exp(5) &amp; x &gt; 5
\end{cases}$$
<strong>方案2：查找表+插值</strong></p>
<ul>
<li>预计算256个点的exp值</li>
<li>线性插值获得中间值</li>
<li>误差控制在0.1%以内</li>
</ul>
<p><strong>4. 数值稳定性增强</strong>：</p>
<p>对于低精度计算，增强数值稳定性：
$$m_i^{new} = \max(m_i^{old}, m_{ij})$$
$$\Delta m = m_i^{new} - m_i^{old}$$
$$l_i^{new} = \begin{cases}
l_i^{old} + l_{ij} &amp; \text{if } \Delta m &lt; \epsilon \\
e^{-\Delta m} \cdot l_i^{old} + l_{ij} &amp; \text{otherwise}
\end{cases}$$</p>
<h4 id="13173">13.1.7.3 平台特定优化</h4>
<p><strong>1. ARM CPU优化</strong>：</p>
<p>利用ARM特定特性：</p>
<ul>
<li>预取指令：提前加载下一个块</li>
<li>NEON内联函数：向量化exp和max操作</li>
<li>大小核调度：compute密集部分用大核，memory密集部分用小核</li>
</ul>
<p><strong>2. Apple Silicon优化</strong>：</p>
<p>利用统一内存架构（UMA）：</p>
<ul>
<li>零拷贝：CPU和GPU共享内存</li>
<li>ANE协处理：将softmax卸载到神经引擎</li>
<li>AMX加速：使用Apple矩阵协处理器</li>
</ul>
<p><strong>3. 移动GPU优化</strong>：</p>
<p>适配移动GPU特点：</p>
<ul>
<li>Warp大小差异：Mali(4), Adreno(32), PowerVR(32)</li>
<li>共享内存bank数：通常为4或8，需要避免bank冲突</li>
<li>精度支持：部分仅支持FP16，需要仿真FP32累加</li>
</ul>
<h4 id="13174">13.1.7.4 能效优化</h4>
<p><strong>1. 动态电压频率调节（DVFS）</strong>：</p>
<p>根据计算特性调整频率：</p>
<ul>
<li>Memory-bound阶段：降低计算单元频率，提高内存频率</li>
<li>Compute-bound阶段：提高计算单元频率</li>
</ul>
<p><strong>2. 异构计算调度</strong>：</p>
<p>| 序列长度 | 推荐硬件 | 原因 |</p>
<table>
<thead>
<tr>
<th>序列长度</th>
<th>推荐硬件</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt;256</td>
<td>CPU</td>
<td>启动开销小，缓存友好</td>
</tr>
<tr>
<td>256-1024</td>
<td>GPU/NPU</td>
<td>并行度适中</td>
</tr>
<tr>
<td>&gt;1024</td>
<td>GPU + CPU协同</td>
<td>GPU处理主体，CPU处理边界</td>
</tr>
</tbody>
</table>
<p><strong>3. 批处理策略</strong>：</p>
<p>边缘设备内存有限，批处理策略需要权衡：</p>
<ul>
<li>小批量（1-4）：减少内存占用，提高实时性</li>
<li>动态批处理：根据当前内存使用情况调整</li>
<li>序列打包：不同长度序列打包减少padding</li>
</ul>
<h4 id="13175">13.1.7.5 实际部署案例分析</h4>
<p><strong>1. llama.cpp的实现</strong>：</p>
<p>在Apple Silicon上的优化：</p>
<ul>
<li>使用Metal Performance Shaders (MPS)</li>
<li>块大小根据模型大小自适应（7B: B=32, 13B: B=64）</li>
<li>利用bfloat16提高数值稳定性</li>
</ul>
<p>性能数据（M2 Max，7B模型）：</p>
<ul>
<li>标准注意力：45 tokens/s</li>
<li>Flash Attention：78 tokens/s (1.73×提升)</li>
<li>内存占用减少35%</li>
</ul>
<p><strong>2. MNN框架实现</strong>：</p>
<p>针对移动设备的优化：</p>
<ul>
<li>自适应精度：根据硬件能力选择FP32/FP16/INT8</li>
<li>内存池管理：避免频繁分配释放</li>
<li>算子融合：将LayerNorm与Attention融合</li>
</ul>
<p>性能数据（Snapdragon 8Gen2，1.8B模型）：</p>
<ul>
<li>预填充延迟：降低40%</li>
<li>解码吞吐量：提升65%</li>
<li>功耗：降低25%</li>
</ul>
<p><strong>3. ONNX Runtime实现</strong>：</p>
<p>跨平台统一接口：</p>
<ul>
<li>运行时选择最优实现</li>
<li>支持QNN（Qualcomm）、NNAPI（Android）、CoreML（iOS）</li>
<li>自动图优化和算子融合</li>
</ul>
<h4 id="13176">13.1.7.6 优化效果评估</h4>
<p><strong>综合性能对比</strong>（1.8B模型，序列长度512）：</p>
<p>| 设备 | 实现方式 | 预填充(ms) | 解码(tokens/s) | 内存(MB) | 功耗(W) |</p>
<table>
<thead>
<tr>
<th>设备</th>
<th>实现方式</th>
<th>预填充(ms)</th>
<th>解码(tokens/s)</th>
<th>内存(MB)</th>
<th>功耗(W)</th>
</tr>
</thead>
<tbody>
<tr>
<td>iPhone 14 Pro</td>
<td>标准注意力</td>
<td>850</td>
<td>12</td>
<td>420</td>
<td>3.2</td>
</tr>
<tr>
<td>iPhone 14 Pro</td>
<td>Flash Attention</td>
<td>520</td>
<td>18</td>
<td>280</td>
<td>2.8</td>
</tr>
<tr>
<td>Pixel 7 Pro</td>
<td>标准注意力</td>
<td>920</td>
<td>10</td>
<td>450</td>
<td>3.5</td>
</tr>
<tr>
<td>Pixel 7 Pro</td>
<td>Flash Attention</td>
<td>580</td>
<td>16</td>
<td>300</td>
<td>3.0</td>
</tr>
<tr>
<td>Jetson Orin</td>
<td>标准注意力</td>
<td>450</td>
<td>22</td>
<td>380</td>
<td>5.0</td>
</tr>
<tr>
<td>Jetson Orin</td>
<td>Flash Attention</td>
<td>280</td>
<td>35</td>
<td>250</td>
<td>4.2</td>
</tr>
</tbody>
</table>
<p><strong>关键洞察</strong>：</p>
<ol>
<li>Flash Attention在边缘设备上的加速比（1.5-1.8×）低于数据中心GPU（2-3×）</li>
<li>内存节省效果显著，对边缘设备尤其重要</li>
<li>功耗降低10-20%，延长电池寿命</li>
<li>实现复杂度较高，需要针对每个平台优化</li>
</ol>
<h2 id="132-multi-querygrouped-query-attention">13.2 Multi-Query/Grouped-Query Attention</h2>
<h3 id="1321">13.2.1 多头注意力的冗余性分析</h3>
<p>标准多头注意力（Multi-Head Attention, MHA）为每个头独立计算 $Q_h, K_h, V_h$：
$$\text{MHA}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_H)W^O$$
其中每个头：
$$\text{head}_h = \text{Attention}(QW_h^Q, KW_h^K, VW_h^V)$$
<strong>参数和计算开销详细分析</strong>：</p>
<p>| 组件 | 参数量 | 计算量（前向） | 内存占用 |</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>参数量</th>
<th>计算量（前向）</th>
<th>内存占用</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q投影</td>
<td>$H \times d_{model} \times d_{head}$</td>
<td>$O(NHd_{model}d_{head})$</td>
<td>批处理时可忽略</td>
</tr>
<tr>
<td>K投影</td>
<td>$H \times d_{model} \times d_{head}$</td>
<td>$O(NHd_{model}d_{head})$</td>
<td>KV cache主要部分</td>
</tr>
<tr>
<td>V投影</td>
<td>$H \times d_{model} \times d_{head}$</td>
<td>$O(NHd_{model}d_{head})$</td>
<td>KV cache主要部分</td>
</tr>
<tr>
<td>注意力计算</td>
<td>0</td>
<td>$O(HN^2d_{head})$</td>
<td>$O(HN^2)$临时存储</td>
</tr>
<tr>
<td>输出投影</td>
<td>$d_{model} \times d_{model}$</td>
<td>$O(Nd_{model}^2)$</td>
<td>可忽略</td>
</tr>
</tbody>
</table>
<p><strong>KV Cache的内存瓶颈分析</strong>：</p>
<p>对于批量推理场景，KV cache占用计算：
$$\text{Memory}_{KV} = 2 \times B \times L \times H \times N \times d_{head} \times \text{sizeof(dtype)}$$
其中：</p>
<ul>
<li>$B$：批大小</li>
<li>$L$：层数</li>
<li>$H$：头数</li>
<li>$N$：序列长度</li>
<li>$d_{head}$：每个头的维度</li>
</ul>
<p><strong>实例计算</strong>（LLaMA-70B）：</p>
<ul>
<li>参数：$L=80, H=64, d_{head}=128$</li>
<li>批大小32，序列长度2048，FP16存储</li>
<li>KV cache大小：$2 \times 32 \times 80 \times 64 \times 2048 \times 128 \times 2 = 168$ GB</li>
</ul>
<p>这远超大多数GPU的显存容量！</p>
<p><strong>冗余性的理论分析</strong>：</p>
<ol>
<li><strong>注意力模式的相似性</strong></li>
</ol>
<p>定义头 $h_i$ 和 $h_j$ 之间的注意力模式相似度：
$$\text{Sim}(h_i, h_j) = \frac{1}{N} \sum_{n=1}^N \text{cos}(A_n^{(i)}, A_n^{(j)})$$
其中 $A_n^{(h)}$ 是头 $h$ 在位置 $n$ 的注意力分布。</p>
<p><strong>实证发现</strong>：</p>
<ul>
<li>相邻层的对应头：相似度 &gt; 0.85</li>
<li>同层相邻头：相似度 &gt; 0.75</li>
<li>随机头对：相似度 ≈ 0.3-0.4</li>
</ul>
<ol start="2">
<li><strong>键值空间的低秩性</strong></li>
</ol>
<p>对键值矩阵进行奇异值分解（SVD）：
$$K = U_K \Sigma_K V_K^T, \quad V = U_V \Sigma_V V_V^T$$
<strong>谱分析结果</strong>：
| 累积方差解释比例 | 所需主成分数 | 相对于原始维度 |</p>
<table>
<thead>
<tr>
<th>累积方差解释比例</th>
<th>所需主成分数</th>
<th>相对于原始维度</th>
</tr>
</thead>
<tbody>
<tr>
<td>80%</td>
<td>5-8</td>
<td>6-10%</td>
</tr>
<tr>
<td>90%</td>
<td>10-15</td>
<td>12-19%</td>
</tr>
<tr>
<td>95%</td>
<td>20-25</td>
<td>25-31%</td>
</tr>
<tr>
<td>99%</td>
<td>40-50</td>
<td>50-62%</td>
</tr>
</tbody>
</table>
<p>这表明键值空间存在显著的低秩结构。</p>
<ol start="3">
<li><strong>信息论视角</strong></li>
</ol>
<p>使用互信息（Mutual Information）分析不同头之间的依赖关系：
$$I(h_i; h_j) = \sum_{a_i, a_j} p(a_i, a_j) \log \frac{p(a_i, a_j)}{p(a_i)p(a_j)}$$
<strong>发现</strong>：</p>
<ul>
<li>大多数头对的互信息很高（&gt;0.6 bits）</li>
<li>表明存在大量冗余信息</li>
<li>可以通过共享减少冗余</li>
</ul>
<ol start="4">
<li><strong>功能特化分析</strong></li>
</ol>
<p>通过分析不同头的激活模式，研究人员发现了功能特化现象：</p>
<p>| 头类型 | 比例 | 功能描述 | 可共享性 |</p>
<table>
<thead>
<tr>
<th>头类型</th>
<th>比例</th>
<th>功能描述</th>
<th>可共享性</th>
</tr>
</thead>
<tbody>
<tr>
<td>位置头</td>
<td>15-20%</td>
<td>关注相对位置</td>
<td>高</td>
</tr>
<tr>
<td>语法头</td>
<td>10-15%</td>
<td>捕获语法结构</td>
<td>中</td>
</tr>
<tr>
<td>语义头</td>
<td>20-25%</td>
<td>语义相似性</td>
<td>低</td>
</tr>
<tr>
<td>稀疏头</td>
<td>30-40%</td>
<td>稀疏激活模式</td>
<td>高</td>
</tr>
<tr>
<td>全局头</td>
<td>5-10%</td>
<td>全局信息聚合</td>
<td>中</td>
</tr>
</tbody>
</table>
<p><strong>优化机会</strong>：</p>
<ol>
<li>位置头和稀疏头高度可共享（约50%的头）</li>
<li>语义头需要保持独立性</li>
<li>可以设计混合策略：部分共享 + 部分独立</li>
</ol>
<h3 id="1322-multi-query-attention-mqa">13.2.2 Multi-Query Attention (MQA)</h3>
<p>MQA的核心思想是<strong>所有头共享同一组键值对</strong>：
$$\text{MQA}(Q, K, V) = \text{Concat}(\text{head}_1^{MQ}, ..., \text{head}_H^{MQ})W^O$$
其中：
$$\text{head}_h^{MQ} = \text{Attention}(Q_h, K_{shared}, V_{shared})$$</p>
<h4 id="13221-mqa">13.2.2.1 MQA的数学原理</h4>
<p><strong>标准MHA到MQA的转换</strong>：</p>
<p>标准MHA中，每个头有独立的键值投影：
$$K_h = XW_h^K, \quad V_h = XW_h^V$$
MQA将所有头的键值投影合并：
$$K_{shared} = X\bar{W}^K, \quad V_{shared} = X\bar{W}^V$$
其中 $\bar{W}^K, \bar{W}^V \in \mathbb{R}^{d_{model} \times d_k}$ 是共享的投影矩阵。</p>
<p><strong>理论基础</strong>：</p>
<p>MQA的有效性基于以下假设：</p>
<ol>
<li><strong>键值空间的共享表示足够</strong>：不同查询头可以从同一键值表示中提取所需信息</li>
<li><strong>查询的多样性保留</strong>：通过保持查询的多头结构，维持模型的表达能力</li>
</ol>
<p><strong>信息瓶颈分析</strong>：</p>
<p>从信息论角度，MQA引入了信息瓶颈：
$$I(X; Y|Q) \leq I(X; K_{shared}, V_{shared})$$
其中 $I(X; Y|Q)$ 是给定查询Q时，输入X和输出Y之间的互信息。</p>
<h4 id="13222">13.2.2.2 实现优化策略</h4>
<p><strong>1. 内存布局优化</strong>：</p>
<p>为了高效广播共享的KV到所有头，需要优化内存布局：</p>
<p>| 布局方案 | 优点 | 缺点 | 适用场景 |</p>
<table>
<thead>
<tr>
<th>布局方案</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>复制扩展</td>
<td>访问模式简单</td>
<td>内存占用增加</td>
<td>小批量推理</td>
</tr>
<tr>
<td>广播索引</td>
<td>内存效率高</td>
<td>需要间接访问</td>
<td>大批量推理</td>
</tr>
<tr>
<td>融合kernel</td>
<td>避免显式广播</td>
<td>实现复杂</td>
<td>高性能需求</td>
</tr>
</tbody>
</table>
<p><strong>2. 计算优化</strong>：</p>
<p><strong>批量矩阵乘法（BMM）优化</strong>：</p>
<ul>
<li>MHA：需要 $H$ 次独立的BMM操作</li>
<li>MQA：可以合并为单次大的BMM操作</li>
</ul>
<p>计算模式对比：</p>
<pre class="codehilite"><code>MHA: 
for h in range(H):
    S_h = Q_h @ K_h.T  # H次小矩阵乘法

MQA:
S_all = Q_all @ K_shared.T  # 1次大矩阵乘法
</code></pre>

<p><strong>3. 硬件适配</strong>：</p>
<p>| 硬件类型 | MQA优化策略 | 性能提升 |</p>
<table>
<thead>
<tr>
<th>硬件类型</th>
<th>MQA优化策略</th>
<th>性能提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPU (Tensor Core)</td>
<td>利用更大的矩阵块</td>
<td>1.5-2×</td>
</tr>
<tr>
<td>CPU (AVX-512)</td>
<td>SIMD广播优化</td>
<td>1.3-1.8×</td>
</tr>
<tr>
<td>NPU/TPU</td>
<td>专用广播单元</td>
<td>2-3×</td>
</tr>
<tr>
<td>移动GPU</td>
<td>减少内存事务</td>
<td>1.8-2.5×</td>
</tr>
</tbody>
</table>
<h4 id="13223-mqa">13.2.2.3 MQA的变体和改进</h4>
<ol>
<li><strong>Multi-Query Attention with Bias (MQA-B)</strong></li>
</ol>
<p>添加可学习的偏置来增强表达能力：
$$\text{head}_h^{MQA-B} = \text{Attention}(Q_h, K_{shared} + B_h^K, V_{shared} + B_h^V)$$
其中 $B_h^K, B_h^V$ 是每个头的偏置向量。</p>
<ol start="2">
<li><strong>Factorized Multi-Query Attention</strong></li>
</ol>
<p>使用低秩分解进一步压缩：
$$K_{shared} = XW_K^{base}W_K^{down}, \quad W_K^{base} \in \mathbb{R}^{d_{model} \times r}, W_K^{down} \in \mathbb{R}^{r \times d_k}$$
其中 $r &lt; d_k$ 是低秩维度。</p>
<ol start="3">
<li><strong>Dynamic Multi-Query Attention</strong></li>
</ol>
<p>根据输入动态调整共享程度：
$$\alpha = \sigma(Xw_{gate})$$
$$K_{dynamic} = \alpha \cdot K_{shared} + (1-\alpha) \cdot K_{specific}$$</p>
<h4 id="13224">13.2.2.4 性能分析</h4>
<p><strong>内存带宽分析</strong>（解码阶段）：</p>
<p>设每个token生成时需要访问的数据量：</p>
<p>| 方法 | KV读取量 | 相对MHA | 带宽需求(GB/s) |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>KV读取量</th>
<th>相对MHA</th>
<th>带宽需求(GB/s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>MHA</td>
<td>$2BLHNd_k$</td>
<td>100%</td>
<td>25.6</td>
</tr>
<tr>
<td>MQA</td>
<td>$2BLNd_k$</td>
<td>3.1%</td>
<td>0.8</td>
</tr>
<tr>
<td>GQA-8</td>
<td>$2BLGNd_k$</td>
<td>12.5%</td>
<td>3.2</td>
</tr>
</tbody>
</table>
<p><em>假设：B=32, L=32, H=32, N=2048, d_k=128, 100 tokens/s</em></p>
<p><strong>计算密度提升</strong>：
$$\text{Arithmetic Intensity}_{MQA} = \frac{\text{FLOPs}}{\text{Memory Access}} = \frac{2BHNd_k}{2BNd_k} = H$$
相比MHA提升了 $H$ 倍的计算密度！</p>
<p><strong>实际测试结果</strong>（A100 GPU，13B模型）：</p>
<p>| 序列长度 | MHA吞吐量 | MQA吞吐量 | 加速比 |</p>
<table>
<thead>
<tr>
<th>序列长度</th>
<th>MHA吞吐量</th>
<th>MQA吞吐量</th>
<th>加速比</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>145 tok/s</td>
<td>287 tok/s</td>
<td>1.98×</td>
</tr>
<tr>
<td>2048</td>
<td>38 tok/s</td>
<td>112 tok/s</td>
<td>2.95×</td>
</tr>
<tr>
<td>8192</td>
<td>9 tok/s</td>
<td>34 tok/s</td>
<td>3.78×</td>
</tr>
</tbody>
</table>
<h4 id="13225">13.2.2.5 质量影响分析</h4>
<p><strong>困惑度（Perplexity）对比</strong>：</p>
<p>| 数据集 | MHA | MQA | 相对退化 |</p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>MHA</th>
<th>MQA</th>
<th>相对退化</th>
</tr>
</thead>
<tbody>
<tr>
<td>WikiText-103</td>
<td>10.82</td>
<td>11.15</td>
<td>+3.0%</td>
</tr>
<tr>
<td>C4</td>
<td>12.45</td>
<td>12.89</td>
<td>+3.5%</td>
</tr>
<tr>
<td>OpenWebText</td>
<td>11.23</td>
<td>11.68</td>
<td>+4.0%</td>
</tr>
</tbody>
</table>
<p><strong>下游任务性能</strong>：</p>
<p>| 任务 | 指标 | MHA | MQA | 差异 |</p>
<table>
<thead>
<tr>
<th>任务</th>
<th>指标</th>
<th>MHA</th>
<th>MQA</th>
<th>差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>Acc</td>
<td>67.8%</td>
<td>66.2%</td>
<td>-1.6%</td>
</tr>
<tr>
<td>HumanEval</td>
<td>Pass@1</td>
<td>32.1%</td>
<td>30.5%</td>
<td>-1.6%</td>
</tr>
<tr>
<td>BBH</td>
<td>Avg</td>
<td>51.2%</td>
<td>49.8%</td>
<td>-1.4%</td>
</tr>
</tbody>
</table>
<p><strong>质量退化的缓解策略</strong>：</p>
<ol>
<li><strong>知识蒸馏</strong>：用MHA教师模型指导MQA学生模型</li>
<li><strong>渐进式转换</strong>：训练过程中逐步增加共享程度</li>
<li><strong>关键层保留MHA</strong>：在重要层（如最后几层）保持MHA</li>
<li><strong>更大的模型规模</strong>：MQA允许在相同资源下训练更大模型</li>
</ol>
<h3 id="1323-grouped-query-attention-gqa">13.2.3 Grouped-Query Attention (GQA)</h3>
<p>GQA是MHA和MQA的折中方案，将查询头分成 $G$ 组，每组共享KV：
$$\text{head}_h^{GQ} = \text{Attention}(Q_h, K_{g(h)}, V_{g(h)})$$
其中 $g(h) = \lfloor h \cdot G / H \rfloor$ 是头 $h$ 所属的组。</p>
<p><strong>设计空间</strong>：</p>
<ul>
<li>$G = 1$：退化为MQA</li>
<li>$G = H$：退化为MHA</li>
<li>典型选择：$G = H/8$ 或 $H/4$</li>
</ul>
<p><strong>KV cache大小</strong>：$2 \times \text{batch} \times G \times N \times d_{head}$</p>
<h3 id="1324">13.2.4 注意力变体的性能分析</h3>
<p><strong>理论分析</strong>（解码阶段）：</p>
<p>设批大小为 $B$，已生成长度为 $N$，则生成一个token的计算量和内存访问：</p>
<p>| 方法 | 计算量 (FLOPs) | KV cache读取 | 内存带宽需求 |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>计算量 (FLOPs)</th>
<th>KV cache读取</th>
<th>内存带宽需求</th>
</tr>
</thead>
<tbody>
<tr>
<td>MHA</td>
<td>$O(BHNd_{head})$</td>
<td>$O(BHNd_{head})$</td>
<td>高</td>
</tr>
<tr>
<td>MQA</td>
<td>$O(BHNd_{head})$</td>
<td>$O(BNd_{head})$</td>
<td>低（减少$H$倍）</td>
</tr>
<tr>
<td>GQA</td>
<td>$O(BHNd_{head})$</td>
<td>$O(BGNd_{head})$</td>
<td>中等</td>
</tr>
</tbody>
</table>
<p><strong>实际性能考虑</strong>：</p>
<ol>
<li>
<p><strong>计算/访存比</strong>：
   - MQA提高了计算密度，更适合memory-bound场景
   - 在边缘设备上效果尤其明显</p>
</li>
<li>
<p><strong>并行度</strong>：
   - MHA可以完全并行计算所有头
   - MQA/GQA需要广播共享的KV，可能影响并行效率</p>
</li>
</ol>
<h3 id="1325-mhamqagqa">13.2.5 从MHA到MQA/GQA的转换</h3>
<p><strong>训练时转换</strong>：</p>
<ol>
<li>
<p><strong>初始化策略</strong>：
   - 平均池化：$K_{shared} = \frac{1}{H}\sum_{h=1}^H K_h$
   - 选择特定头：$K_{shared} = K_1$（通常选择第一个头）</p>
</li>
<li>
<p><strong>渐进式转换</strong>：</p>
</li>
</ol>
<pre class="codehilite"><code>α = min(1, training_step / warmup_steps)
K_effective = α * K_shared + (1-α) * K_original
</code></pre>

<p><strong>推理时近似</strong>（无需重训练）：</p>
<ol>
<li>
<p><strong>平均池化方法</strong>：
$$K_{MQA} = \frac{1}{H}\sum_{h=1}^H K_h^{MHA}, \quad V_{MQA} = \frac{1}{H}\sum_{h=1}^H V_h^{MHA}$$</p>
</li>
<li>
<p><strong>主成分分析（PCA）</strong>：
   - 对 $[K_1, ..., K_H]$ 进行SVD分解
   - 取最大奇异值对应的向量作为共享KV</p>
</li>
</ol>
<h3 id="1326">13.2.6 边缘部署的实践考虑</h3>
<ol>
<li>
<p><strong>内存布局优化</strong>：
   - MQA/GQA的KV需要高效的广播机制
   - 考虑SIMD指令的对齐要求</p>
</li>
<li>
<p><strong>量化策略</strong>：
   - 共享KV可能需要更高的量化精度
   - Per-head量化 vs Per-tensor量化的权衡</p>
</li>
<li>
<p><strong>动态切换</strong>：
   - 根据序列长度动态选择MHA/GQA
   - 短序列用MHA保持质量，长序列用GQA节省内存</p>
</li>
</ol>
<h3 id="1327">13.2.7 实验结果与分析</h3>
<p>以LLaMA系列模型为例：</p>
<p>| 模型 | 原始(MHA) | GQA-8 | GQA-4 | MQA |</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>原始(MHA)</th>
<th>GQA-8</th>
<th>GQA-4</th>
<th>MQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>PPL提升</td>
<td>0%</td>
<td>+0.2%</td>
<td>+0.5%</td>
<td>+1.2%</td>
</tr>
<tr>
<td>KV cache</td>
<td>100%</td>
<td>12.5%</td>
<td>25%</td>
<td>3.1%</td>
</tr>
<tr>
<td>解码速度提升</td>
<td>1x</td>
<td>1.8x</td>
<td>1.5x</td>
<td>2.2x</td>
</tr>
</tbody>
</table>
<p>关键发现：</p>
<ol>
<li>GQA-8（8组）在质量损失很小的情况下获得显著加速</li>
<li>MQA在极长序列（&gt;4K）时优势最明显</li>
<li>不同任务对共享KV的敏感度不同，知识密集型任务损失较大</li>
</ol>
<h2 id="133">13.3 稀疏注意力模式</h2>
<h3 id="1331">13.3.1 注意力稀疏性的动机</h3>
<p>完整注意力的 $O(N^2)$ 复杂度在长序列上变得不可承受。然而，实证研究表明：</p>
<ol>
<li><strong>注意力分布的长尾特性</strong>：大部分注意力权重接近于0</li>
<li><strong>局部性</strong>：相邻token之间的注意力通常更强</li>
<li><strong>全局锚点</strong>：某些特殊token（如[CLS]）需要全局信息</li>
</ol>
<p>这些观察启发了各种稀疏注意力模式的设计。</p>
<h3 id="1332">13.3.2 固定稀疏模式</h3>
<ol>
<li><strong>窗口注意力（Window Attention）</strong></li>
</ol>
<p>每个token只关注固定窗口内的其他token：
$$S_{ij} = \begin{cases}
\frac{Q_iK_j^T}{\sqrt{d_k}} &amp; \text{if } |i-j| \leq w \\
-\infty &amp; \text{otherwise}
\end{cases}$$
其中 $w$ 是窗口半径。</p>
<p><strong>复杂度</strong>：$O(Nw)$ 而非 $O(N^2)$</p>
<ol start="2">
<li><strong>跨步注意力（Strided Attention）</strong></li>
</ol>
<p>固定步长的稀疏连接：
$$\text{mask}(i,j) = \mathbb{1}[(i-j) \bmod s = 0]$$</p>
<ol start="3">
<li><strong>组合模式（Combination Patterns）</strong></li>
</ol>
<p>Longformer提出的组合模式：</p>
<ul>
<li>滑动窗口注意力（局部）</li>
<li>扩张窗口注意力（中等距离）</li>
<li>全局注意力（特定token）</li>
</ul>
<p>数学表示：
$$\text{Attention}_i = \begin{cases}
\text{WindowAttn}_i &amp; \text{if } i \in \text{LocalTokens} \\
\text{GlobalAttn}_i &amp; \text{if } i \in \text{GlobalTokens}
\end{cases}$$</p>
<h3 id="1333">13.3.3 学习型稀疏模式</h3>
<ol>
<li><strong>基于阈值的动态稀疏</strong></li>
</ol>
<p>计算完整注意力分数后，保留top-k或超过阈值的连接：
$$P_{ij} = \begin{cases}
\text{softmax}(S_{ij}) &amp; \text{if } S_{ij} \in \text{top-k}(S_i) \\
0 &amp; \text{otherwise}
\end{cases}$$
<strong>问题</strong>：需要先计算完整的 $S = QK^T$，无法节省计算</p>
<ol start="2">
<li><strong>可学习的稀疏掩码</strong></li>
</ol>
<p>引入可学习的二值掩码 $M \in \{0,1\}^{N \times N}$：
$$\text{SparseAttn}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} \odot M\right)V$$
使用Gumbel-Softmax或直通估计器（STE）进行训练。</p>
<h3 id="1334">13.3.4 分层稀疏注意力</h3>
<p><strong>BigBird的设计</strong>：结合三种注意力模式</p>
<ol>
<li><strong>随机注意力</strong>：每个token随机关注 $r$ 个其他token</li>
<li><strong>窗口注意力</strong>：关注邻近的 $w$ 个token  </li>
<li><strong>全局注意力</strong>：$g$ 个全局token与所有其他token相连</li>
</ol>
<p>总的连接数：$O(N(r + w + g))$</p>
<p><strong>数学形式化</strong>：
设注意力图 $G = (V, E)$，其中：</p>
<ul>
<li>$E_{window} = \{(i,j) : |i-j| \leq w\}$</li>
<li>$E_{random} = \{(i,j) : (i,j) \in \text{RandomSample}(r)\}$</li>
<li>$E_{global} = \{(i,j) : i \in G \text{ or } j \in G\}$</li>
</ul>
<p>则 $E = E_{window} \cup E_{random} \cup E_{global}$</p>
<h3 id="1335">13.3.5 稀疏注意力的高效实现</h3>
<ol>
<li><strong>块稀疏格式（Block Sparse Format）</strong></li>
</ol>
<p>将注意力矩阵分成 $B \times B$ 的块，只计算非零块：</p>
<pre class="codehilite"><code>稀疏掩码（块级别）：
[1 1 0 0]
[1 1 1 0]  
[0 1 1 1]
[0 0 1 1]
</code></pre>

<p><strong>优势</strong>：</p>
<ul>
<li>更好的硬件利用率（块级别的矩阵乘法）</li>
<li>减少索引开销</li>
</ul>
<ol start="2">
<li><strong>CSR格式存储</strong></li>
</ol>
<p>对于极度稀疏的模式，使用压缩稀疏行（CSR）格式：</p>
<ul>
<li>values: 非零元素值</li>
<li>col_indices: 列索引</li>
<li>row_ptr: 每行的起始位置</li>
</ul>
<ol start="3">
<li><strong>核融合优化</strong></li>
</ol>
<p>避免生成完整的注意力矩阵：</p>
<pre class="codehilite"><code>for each query i:
    sparse_indices = get_sparse_pattern(i)
    K_sparse = gather(K, sparse_indices)
    S_sparse = Q[i] @ K_sparse.T
    P_sparse = softmax(S_sparse)
    O[i] = P_sparse @ gather(V, sparse_indices)
</code></pre>

<h3 id="1336">13.3.6 稀疏模式的选择策略</h3>
<p><strong>1. 基于任务的选择</strong>：
- 文档理解：需要长距离依赖，适合BigBird模式
- 对话生成：局部连贯性重要，窗口注意力足够
- 代码生成：需要结构化稀疏（如语法树引导）</p>
<p><strong>2. 基于硬件的选择</strong>：
- GPU：块稀疏模式，利用tensor core
- CPU：CSR格式，优化缓存访问
- DSP：固定模式，便于向量化</p>
<p><strong>3. 动态选择</strong>：
根据序列长度动态切换：</p>
<pre class="codehilite"><code>if seq_len &lt; 512:
    use_full_attention()
elif seq_len &lt; 2048:
    use_window_attention(window=256)
else:
    use_bigbird_attention()
</code></pre>

<h3 id="1337">13.3.7 稀疏注意力的理论保证</h3>
<p><strong>表达能力分析</strong>：</p>
<p>定理：对于窗口大小 $w = O(\log N)$，$L$ 层的窗口注意力可以模拟完整注意力。</p>
<p>证明要点：</p>
<ul>
<li>每层扩展感受野 $2w$</li>
<li>$L$ 层后的感受野：$2Lw = O(L\log N)$</li>
<li>当 $L = O(N/\log N)$ 时可覆盖整个序列</li>
</ul>
<p><strong>近似误差界</strong>：</p>
<p>对于top-k稀疏：
$$|P_{full} - P_{sparse}|_F \leq \epsilon$$
其中 $k = O(N\log N/\epsilon^2)$ 即可保证 $\epsilon$ 误差。</p>
<h2 id="134">13.4 线性注意力机制</h2>
<h3 id="1341">13.4.1 线性注意力的核心思想</h3>
<p>标准注意力的计算瓶颈在于 $\text{softmax}(QK^T)$ 的矩阵乘法。线性注意力通过<strong>分解注意力矩阵</strong>来避免显式计算 $N \times N$ 的矩阵。</p>
<p><strong>核心变换</strong>：
将 $\text{softmax}(QK^T)$ 近似为 $\phi(Q)\phi(K)^T$，其中 $\phi$ 是特征映射。</p>
<p>这样可以改变计算顺序：
$$\text{Attention}(Q,K,V) = \phi(Q)[\phi(K)^TV]$$
计算顺序的改变将复杂度从 $O(N^2d)$ 降至 $O(Nd^2)$。</p>
<h3 id="1342">13.4.2 核方法视角</h3>
<p><strong>Softmax作为核函数</strong>：</p>
<p>标准注意力可以写成：
$$A_{ij} = \frac{\exp(Q_iK_j^T/\sqrt{d})}{\sum_k \exp(Q_iK_k^T/\sqrt{d})} = \frac{k(Q_i, K_j)}{\sum_k k(Q_i, K_k)}$$
其中 $k(x,y) = \exp(x^Ty/\sqrt{d})$ 是指数核。</p>
<p><strong>核函数的分解</strong>：</p>
<p>如果存在特征映射 $\phi$ 使得：
$$k(x,y) = \langle\phi(x), \phi(y)\rangle$$
则可以实现线性复杂度的注意力。</p>
<h3 id="1343">13.4.3 具体的线性注意力方法</h3>
<ol>
<li><strong>Linear Transformer (Katharopoulos et al.)</strong></li>
</ol>
<p>使用简单的特征映射：
$$\phi(x) = \text{elu}(x) + 1$$
其中elu是指数线性单元。这保证了 $\phi(x) \geq 0$。</p>
<p><strong>因果掩码的处理</strong>：
对于自回归生成，需要因果掩码。线性注意力通过RNN形式实现：
$$S_i = S_{i-1} + \phi(K_i)V_i^T$$
$$O_i = \frac{\phi(Q_i)S_i}{\phi(Q_i)\sum_{j \leq i}\phi(K_j)}$$</p>
<ol start="2">
<li><strong>Performer (Choromanski et al.)</strong></li>
</ol>
<p>使用随机特征近似softmax核：
$$\phi(x) = \frac{\exp(|x|^2/2)}{\sqrt{m}} [\exp(w_1^Tx), ..., \exp(w_m^Tx)]^T$$
其中 $w_i \sim \mathcal{N}(0, I)$ 是随机投影向量。</p>
<p><strong>理论保证</strong>：
当 $m = O(d\log d/\epsilon^2)$ 时，近似误差小于 $\epsilon$。</p>
<ol start="3">
<li><strong>RFA (Random Feature Attention)</strong></li>
</ol>
<p>使用确定性的正交随机特征：</p>
<ol>
<li>生成正交矩阵 $W \in \mathbb{R}^{d \times m}$</li>
<li>$\phi(x) = [\sin(Wx), \cos(Wx)]^T / \sqrt{m}$</li>
</ol>
<p>优势：更稳定的近似，更少的随机性。</p>
<h3 id="1344">13.4.4 线性注意力的统一框架</h3>
<p><strong>一般形式</strong>：
$$\text{LinearAttn}(Q,K,V) = \frac{\phi(Q)[\phi(K)^TV]}{\phi(Q)[\phi(K)^T\mathbf{1}]}$$
其中分母项用于归一化。</p>
<p><strong>设计空间</strong>：</p>
<ol>
<li>
<p><strong>特征映射选择</strong>：
   - 恒等映射：$\phi(x) = x$（需要非负约束）
   - ReLU族：$\phi(x) = \text{ReLU}(x)$
   - 指数族：$\phi(x) = \exp(x/\tau)$
   - 随机特征：如Performer</p>
</li>
<li>
<p><strong>归一化策略</strong>：
   - L2归一化：$\phi(x) = x/|x|_2$
   - Softmax归一化：在特征维度上
   - 无归一化：依赖训练时的正则化</p>
</li>
</ol>
<h3 id="1345">13.4.5 线性注意力的优化技巧</h3>
<ol>
<li><strong>数值稳定性</strong></li>
</ol>
<p>问题：当 $\phi(K)^T\mathbf{1}$ 接近0时，除法不稳定。</p>
<p>解决方案：</p>
<ul>
<li>添加小常数：$\phi(Q)[\phi(K)^T\mathbf{1}] + \epsilon$</li>
<li>使用对数空间计算</li>
<li>梯度裁剪</li>
</ul>
<ol start="2">
<li><strong>特征维度选择</strong></li>
</ol>
<p>权衡：</p>
<ul>
<li>更高维度 $m$：更好的近似，更多计算</li>
<li>建议：$m = O(\sqrt{N})$ 时达到计算-精度平衡</li>
</ul>
<ol start="3">
<li><strong>混合精度策略</strong></li>
</ol>
<ul>
<li>特征映射用FP32计算（数值稳定性）</li>
<li>矩阵乘法用FP16/BF16（速度）</li>
<li>累加器用FP32（精度）</li>
</ul>
<h3 id="1346">13.4.6 线性注意力的应用场景</h3>
<ol>
<li><strong>超长序列处理</strong></li>
</ol>
<p>当 $N \gg d$ 时，线性注意力优势明显：</p>
<ul>
<li>标准注意力：$O(N^2d)$</li>
<li>线性注意力：$O(Nd^2)$</li>
</ul>
<p>临界点：$N = d$ 时两者计算量相当。</p>
<ol start="2">
<li><strong>流式/在线推理</strong></li>
</ol>
<p>RNN形式的线性注意力支持：</p>
<ul>
<li>常数内存的增量计算</li>
<li>无需存储完整的KV cache</li>
<li>适合边缘设备的实时应用</li>
</ul>
<ol start="3">
<li><strong>跨模态注意力</strong></li>
</ol>
<p>图像-文本等跨模态场景，序列长度差异大：</p>
<ul>
<li>图像：$N_{img} = 14 \times 14 = 196$（ViT）</li>
<li>文本：$N_{text} = 512$</li>
<li>交叉注意力：$O(N_{img} \times N_{text})$ → $O((N_{img} + N_{text})d)$</li>
</ul>
<h3 id="1347">13.4.7 实验结果与分析</h3>
<p><strong>在不同任务上的表现</strong>：</p>
<p>| 方法 | 语言建模 (PPL) | 图像分类 (Acc) | 长文本QA (F1) |</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>语言建模 (PPL)</th>
<th>图像分类 (Acc)</th>
<th>长文本QA (F1)</th>
</tr>
</thead>
<tbody>
<tr>
<td>标准注意力</td>
<td>15.2</td>
<td>81.5%</td>
<td>73.4</td>
</tr>
<tr>
<td>Performer</td>
<td>16.1 (+6%)</td>
<td>80.8%</td>
<td>71.2</td>
</tr>
<tr>
<td>Linear Transformer</td>
<td>16.8 (+10%)</td>
<td>79.6%</td>
<td>69.8</td>
</tr>
<tr>
<td>混合方案*</td>
<td>15.5 (+2%)</td>
<td>81.2%</td>
<td>72.9</td>
</tr>
</tbody>
</table>
<p>*混合方案：前几层用标准注意力，后续层用线性注意力</p>
<p><strong>关键发现</strong>：</p>
<ol>
<li>线性注意力在局部依赖任务上表现良好</li>
<li>全局推理任务（如算术）性能下降明显</li>
<li>混合架构能够较好平衡效率和性能</li>
</ol>
<h2 id="_1">本章小结</h2>
<p>本章系统地探讨了注意力机制的各种优化技术，这些技术对于在资源受限的边缘设备上部署大语言模型至关重要：</p>
<ol>
<li>
<p><strong>Flash Attention</strong>通过平铺和重计算策略，将内存访问从 $O(N^2)$ 降至 $O(N)$，在保持精确计算的同时大幅提升了硬件利用率。其核心在于利用GPU的内存层次结构，通过分块计算和增量softmax避免了中间结果的频繁读写。</p>
</li>
<li>
<p><strong>Multi-Query和Grouped-Query Attention</strong>通过在多个查询头之间共享键值对，将KV cache的大小降低了数倍到数十倍。这种方法特别适合解码阶段和长序列场景，在质量损失很小的情况下获得了显著的加速。</p>
</li>
<li>
<p><strong>稀疏注意力模式</strong>利用了注意力分布的稀疏性，通过固定模式（窗口、跨步）或学习型模式将计算复杂度从 $O(N^2)$ 降至 $O(N\log N)$ 或 $O(N)$。BigBird等方法通过组合局部、随机和全局注意力，在保持模型表达能力的同时实现了高效计算。</p>
</li>
<li>
<p><strong>线性注意力机制</strong>通过核方法和特征映射，将注意力计算的复杂度降至 $O(Nd^2)$。虽然在某些任务上有性能损失，但其常数内存的特性使其特别适合流式处理和超长序列。</p>
</li>
</ol>
<p><strong>关键公式回顾</strong>：</p>
<ul>
<li>
<p>Flash Attention的增量softmax更新：
$$O_i^{(j)} = \frac{e^{m_i^{(j-1)} - m_i^{(j)}} l_i^{(j-1)}}{l_i^{(j)}} O_i^{(j-1)} + \frac{1}{l_i^{(j)}} \sum_k e^{S_{ijk} - m_i^{(j)}} V_{jk}$$</p>
</li>
<li>
<p>GQA的头分组映射：
$$g(h) = \lfloor h \cdot G / H \rfloor$$</p>
</li>
<li>
<p>线性注意力的核心变换：
$$\text{Attention}(Q,K,V) = \phi(Q)[\phi(K)^TV]$$
<strong>实践建议</strong>：</p>
</li>
</ul>
<ol>
<li>对于边缘部署，优先考虑GQA（特别是8组配置），它提供了最佳的质量-效率平衡</li>
<li>Flash Attention在有足够SRAM的设备上效果最好，需要根据硬件调整块大小</li>
<li>稀疏注意力适合特定的应用场景，需要根据任务特性选择合适的稀疏模式</li>
<li>线性注意力可以作为混合架构的一部分，在模型的高层使用以处理长距离依赖</li>
</ol>
<h2 id="_2">练习题</h2>
<h3 id="_3">基础题</h3>
<ol>
<li><strong>Flash Attention的内存访问分析</strong>
   计算标准注意力和Flash Attention在序列长度N=2048、特征维度d=64、块大小B=64时的HBM访问次数。假设使用FP16存储。</li>
</ol>
<p><em>Hint</em>：考虑每个矩阵元素的读写次数，以及中间结果的存储。</p>
<ol start="2">
<li><strong>MQA的KV Cache计算</strong>
   对于一个32头的模型，批大小B=8，序列长度N=1024，每个头维度为128，计算MHA、GQA-8和MQA的KV cache内存占用（以MB为单位）。</li>
</ol>
<p><em>Hint</em>：KV cache = 2 × batch × heads × seq_len × head_dim × bytes_per_element</p>
<ol start="3">
<li><strong>稀疏注意力的连接数</strong>
   对于BigBird注意力，如果窗口大小w=3，随机连接数r=2，全局token数g=2，序列长度N=512，计算总的注意力连接数和稀疏度。</li>
</ol>
<p><em>Hint</em>：稀疏度 = 1 - (实际连接数 / N²)</p>
<h3 id="_4">挑战题</h3>
<ol start="4">
<li><strong>Flash Attention的最优块大小</strong>
   给定SRAM大小为48KB，需要存储Q块、K块、V块以及中间结果。在FP16精度下，推导使SRAM利用率最大化的块大小公式。考虑需要额外存储每行的最大值和求和结果。</li>
</ol>
<p><em>Hint</em>：设块大小为B_r × B_c，列出所有需要存储的张量及其大小。</p>
<ol start="5">
<li><strong>线性注意力的误差界分析</strong>
   证明：对于Performer使用m个随机特征时，注意力矩阵的近似误差期望满足：
$$\mathbb{E}[|\hat{A} - A|_F] \leq \frac{C}{\sqrt{m}} |A|_F$$
   其中C是与维度相关的常数。</li>
</ol>
<p><em>Hint</em>：使用随机特征的方差分析和矩阵范数的性质。</p>
<ol start="6">
<li><strong>混合注意力架构设计</strong>
   设计一个12层Transformer的注意力配置，要求：</li>
</ol>
<ul>
<li>前4层保持完整注意力以捕获局部模式</li>
<li>中间4层使用GQA-4以平衡效率</li>
<li>最后4层使用窗口注意力（窗口256）+ 线性注意力的混合</li>
</ul>
<p>分析这种设计在不同序列长度（512, 2048, 8192）下相对于全MHA的计算节省和内存节省。</p>
<p><em>Hint</em>：分别计算每种配置的FLOPs和内存占用，考虑批大小的影响。</p>
<ol start="7">
<li><strong>稀疏模式的表达能力</strong>
   考虑一个只使用窗口大小为w的局部注意力的L层Transformer。如果要保证任意两个位置的信息能够交互，w和L需要满足什么关系？对于序列长度N=1024，如果限制w≤32，最少需要多少层？</li>
</ol>
<p><em>Hint</em>：考虑信息传播的"感受野"概念。</p>
<ol start="8">
<li><strong>注意力优化的能效分析</strong>
   假设一个边缘设备的内存带宽为25.6 GB/s，计算性能为2 TFLOPS（FP16）。对于批大小1、序列长度512、模型维度768的注意力计算，分析标准注意力、Flash Attention和GQA-8分别是compute-bound还是memory-bound。计算各自的硬件利用率。</li>
</ol>
<p><em>Hint</em>：计算arithmetic intensity（FLOPs/字节），与硬件的计算/带宽比值对比。</p>
<h3 id="_5">答案</h3>
<details>
<summary>点击查看答案</summary>
<ol>
<li><strong>标准注意力</strong>：
   - 读Q,K,V: 3×N×d×2 = 3×2048×64×2 = 786KB
   - 写S=QK^T: N²×2 = 8MB
   - 读S计算softmax: 8MB
   - 写P: 8MB
   - 读P,V计算输出: 8MB + 256KB
   - 写输出: 256KB
   - 总计：约32.5MB</li>
</ol>
<p><strong>Flash Attention</strong>：</p>
<ul>
<li>读Q,K,V各一次: 786KB</li>
<li>写输出: 256KB</li>
<li>总计：约1MB</li>
</ul>
<ol start="2">
<li>
<p>MHA: 2×8×32×1024×128×2 = 128MB
   GQA-8: 2×8×8×1024×128×2 = 32MB
   MQA: 2×8×1×1024×128×2 = 4MB</p>
</li>
<li>
<p>每个token的连接数：
   - 窗口: 2w+1 = 7
   - 随机: r = 2
   - 全局: 2g = 4（双向）
   - 总计: 13连接/token（考虑重复）
   - 总连接数: ≈512×13 = 6656
   - 稀疏度: 1 - 6656/(512²) ≈ 97.5%</p>
</li>
<li>
<p>需要存储：
   - Q块: B_r × d × 2字节
   - K,V块: 2 × B_c × d × 2字节
   - S块: B_r × B_c × 2字节
   - 统计量: B_r × 8字节（max和sum各用FP32）</p>
</li>
</ol>
<p>约束：2B_r×d + 4B_c×d + 2B_r×B_c + 8B_r ≤ 48KB</p>
<p>当B_r = B_c = B时，简化为：6Bd + 2B² + 8B ≤ 48KB
   对于d=64，最优B ≈ 48</p>
<ol start="5">
<li>
<p>证明思路：
   - Performer的特征映射引入的误差来自随机投影
   - 每个随机特征的方差为O(1/m)
   - 使用矩阵集中不等式和Frobenius范数的次可加性
   - 最终得到期望误差界O(1/√m)</p>
</li>
<li>
<p>计算节省（以N=2048为例）：
   - 层1-4：标准MHA，100%计算
   - 层5-8：GQA-4，KV cache减少75%，解码加速约1.6x
   - 层9-12：窗口(256) + 线性混合，计算复杂度O(N×256 + N×d²)
   - 总体计算节省：约45%
   - 内存节省：约60%（主要来自KV cache）</p>
</li>
<li>
<p>每层扩展感受野2w，L层后感受野为2Lw
   需要：2Lw ≥ N-1
   因此：L ≥ (N-1)/(2w)</p>
</li>
</ol>
<p>对于N=1024, w=32：L ≥ 1023/64 ≈ 16层</p>
<ol start="8">
<li><strong>标准注意力</strong>：
   - 计算量：4×N²×d = 4×512²×768 ≈ 0.8 GFLOPS
   - 内存访问：约20MB
   - Arithmetic Intensity：0.8G/20M ≈ 40 FLOPs/byte
   - 硬件AI：2T/25.6G ≈ 78 FLOPs/byte
   - 结论：Memory-bound，利用率约51%</li>
</ol>
<p><strong>Flash Attention</strong>：</p>
<ul>
<li>内存访问减少到约2MB</li>
<li>AI提升到约400 FLOPs/byte</li>
<li>结论：Compute-bound，利用率约40%</li>
</ul>
<p><strong>GQA-8</strong>：</p>
<ul>
<li>KV读取减少8倍</li>
<li>解码阶段更接近compute-bound</li>
<li>利用率提升到约70%</li>
</ul>
</details>
            </article>
            
            <nav class="page-nav"><a href="chapter12.html" class="nav-link prev">← 第12章：知识蒸馏</a><a href="chapter14.html" class="nav-link next">第14章：KV Cache管理与压缩 →</a></nav>
        </main>
    </div>
</body>
</html>