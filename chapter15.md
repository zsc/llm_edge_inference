# 第15章：解码加速技术

在大语言模型的推理过程中，自回归解码是最主要的计算瓶颈之一。由于每个token的生成都依赖于之前所有token，这种串行特性严重限制了推理速度。本章深入探讨解码加速技术，包括投机解码、多token预测等方法，这些技术通过巧妙的算法设计将部分串行计算转化为并行计算，在保持生成质量的同时显著提升推理速度。我们将从理论分析出发，详细推导各种加速方法的数学原理，并讨论在边缘设备上的实际应用。

## 15.1 投机解码（Speculative Decoding）原理

### 15.1.1 自回归解码的瓶颈分析

在标准的自回归解码过程中，生成长度为T的序列需要T次前向传播：

对于模型p(x_t|x_{<t})，生成过程为：
- t=1: x_1 ~ p(x_1|x_0)
- t=2: x_2 ~ p(x_2|x_0, x_1)
- ...
- t=T: x_T ~ p(x_T|x_0, ..., x_{T-1})

每次前向传播的计算复杂度为O(n²d)（其中n为序列长度，d为模型维度），导致：
- 总计算复杂度：O(T·n²d)
- 总时间复杂度：O(T)（假设并行计算）
- GPU利用率：通常仅20-30%（batch size=1时）

关键观察：
1. **计算资源未充分利用**：单token生成无法充分利用GPU的并行计算能力
2. **内存带宽瓶颈**：频繁的模型权重加载导致内存带宽成为瓶颈
3. **延迟累积效应**：长序列生成时，延迟线性累积

### 15.1.2 投机解码的数学框架

投机解码的核心思想是使用一个小而快的草稿模型q(x)来生成候选序列，然后用目标模型p(x)进行验证和校正。

**定义：**
- 目标模型：p(x_t|x_{<t})，参数量大，推理慢
- 草稿模型：q(x_t|x_{<t})，参数量小，推理快
- 草稿长度：γ（一次生成的候选token数）

**投机采样算法：**

1. **草稿生成阶段：**
   使用草稿模型生成γ个候选token：
   ```
   对于 i = 1 到 γ:
       x̃_i ~ q(x_i|x_{<i})
   ```

2. **并行验证阶段：**
   目标模型并行计算所有位置的概率：
   ```
   p_i = p(x̃_i|x_{<i}) 对所有 i ∈ [1, γ]
   q_i = q(x̃_i|x_{<i}) 对所有 i ∈ [1, γ]
   ```

3. **接受-拒绝采样：**
   对每个候选token x̃_i，计算接受概率：
   ```
   α_i = min(1, p_i/q_i)
   ```
   
   采样决策：
   - 以概率α_i接受x̃_i
   - 以概率1-α_i拒绝并重新采样

4. **分布校正：**
   当token被拒绝时，从校正分布中采样：
   ```
   p'(x) = norm(max(0, p(x) - q(x)))
   ```
   其中norm表示归一化操作。

**理论保证：**
投机解码保证生成的token序列的分布与目标模型完全一致：

证明概要：
对于任意token x，其被采样的概率为：
```
P(X = x) = q(x)·min(1, p(x)/q(x)) + I[rejected]·p'(x)
         = min(q(x), p(x)) + I[rejected]·(p(x) - q(x))⁺/Z
         = p(x)
```

其中I[rejected]是拒绝指示函数，Z是归一化常数。

### 15.1.3 理论加速比分析

定义关键指标：
- τ_p：目标模型单次推理时间
- τ_q：草稿模型单次推理时间
- γ：草稿长度
- α：平均接受率

**期望接受token数：**
假设每个位置的接受率独立，则期望接受数为：
```
E[n_accept] = Σ_{i=1}^{γ} α^i = α(1-α^γ)/(1-α)
```

**加速比推导：**
投机解码的期望运行时间：
```
T_spec = γ·τ_q + τ_p
```

标准解码生成E[n_accept]个token的时间：
```
T_standard = E[n_accept]·τ_p
```

理论加速比：
```
Speedup = T_standard/T_spec = [α(1-α^γ)/(1-α)]·τ_p / (γ·τ_q + τ_p)
```

**最优草稿长度：**
对加速比关于γ求导，可得最优草稿长度满足：
```
γ_opt ≈ -log(α)/log(1-α) · (τ_p/τ_q - 1)
```

关键洞察：
1. 当α→1时（草稿模型质量高），γ_opt→∞
2. 当τ_q<<τ_p时（草稿模型足够快），可以使用更长的草稿
3. 实践中，γ通常选择4-8

### 15.1.4 实现细节与优化

**批量验证策略：**
将多个候选序列组织为批次进行验证：
```
输入形状：[batch_size, seq_len + γ, hidden_dim]
注意力掩码：因果掩码 + 投机位置掩码
```

优化技巧：
1. **KV Cache复用**：草稿生成和验证阶段共享前缀的KV Cache
2. **提前终止**：当连续k个token被拒绝时，停止验证剩余token
3. **动态草稿长度**：根据历史接受率动态调整γ

**内存管理优化：**
投机解码需要额外存储：
- 草稿模型的KV Cache：O(γ·d_draft)
- 候选token概率：O(γ·vocab_size)
- 验证批次缓冲：O(γ·d_target)

优化策略：
1. 使用环形缓冲区管理候选序列
2. 草稿模型使用INT8量化减少内存占用
3. 概率缓存只保留top-k候选

**硬件适配考虑：**
- GPU：利用Tensor Core加速批量矩阵运算
- CPU：使用SIMD指令加速概率计算
- 移动设备：草稿模型部署在DSP/NPU上

## 15.2 多Token预测（Multi-token Prediction）

### 15.2.1 从单Token到多Token

传统语言模型训练目标：
```
L_single = -Σ_t log p(x_t|x_{<t})
```

这种逐token预测存在的问题：
1. **局部视角**：模型过度关注下一个token，缺乏长程规划
2. **训练-推理不匹配**：训练时看到真实token，推理时使用生成token
3. **计算效率低**：无法并行生成多个token

多Token预测的核心思想是让模型同时预测未来k个token：
```
L_multi = -Σ_t Σ_{i=1}^k log p(x_{t+i}|x_{≤t})
```

**信息论视角：**
从信息论角度，多token预测相当于建模更高阶的条件熵：
```
H(X_{t+1:t+k}|X_{≤t}) ≤ Σ_{i=1}^k H(X_{t+i}|X_{≤t})
```

等号成立当且仅当未来token条件独立。实际上，自然语言中存在强依赖关系，因此联合建模可以获得更好的压缩率。

### 15.2.2 多Token预测架构

**共享编码器设计：**
```
基础架构：
h_t = Transformer(x_{≤t})

多头预测：
对于 i ∈ [1, k]:
    z_{t,i} = ProjectionHead_i(h_t)
    p(x_{t+i}|x_{≤t}) = Softmax(z_{t,i})
```

**联合概率建模：**
方法1：独立预测（最简单）
```
p(x_{t+1:t+k}|x_{≤t}) = Π_{i=1}^k p(x_{t+i}|x_{≤t})
```

方法2：自回归分解
```
p(x_{t+1:t+k}|x_{≤t}) = Π_{i=1}^k p(x_{t+i}|x_{≤t}, x_{t+1:t+i-1})
```

方法3：完全联合（计算复杂度O(V^k)）
```
p(x_{t+1:t+k}|x_{≤t}) = Softmax(MLP(h_t)) ∈ R^{V^k}
```

**实用架构选择：**
考虑到计算效率，通常采用混合方案：
1. 前n个token独立预测（n=2-4）
2. 后续token使用轻量级自回归
3. 共享底层表示，独立预测头

### 15.2.3 训练策略与优化

**损失函数设计：**
基础多token损失：
```
L = Σ_{i=1}^k λ_i · L_i
```
其中L_i是第i个位置的交叉熵损失，λ_i是位置权重。

**权重策略：**
1. 均匀权重：λ_i = 1/k
2. 指数衰减：λ_i = β^{i-1}，β ∈ (0,1)
3. 学习权重：λ_i通过元学习获得

**梯度平衡技术：**
多任务学习中的梯度冲突问题：
```
g_shared = Σ_i ∇_θ L_i
```

当不同位置的梯度方向冲突时，使用梯度投影：
```
g_i^proj = g_i - (g_i·g_j)/(||g_j||²)·g_j
```

**课程学习策略：**
1. 渐进增加预测长度：
   - 初期：k=1（标准训练）
   - 中期：k=2-4
   - 后期：k=4-8

2. 难度自适应调整：
   ```
   k_t = min(k_max, k_0 + α·epoch)
   ```

3. 基于验证性能的动态调整

### 15.2.4 推理时的应用

**并行候选生成：**
利用多token预测能力生成候选树：
```
第1层：预测k个可能的x_1
第2层：对每个x_1，预测k个可能的x_2
...
深度d的树包含k^d个候选序列
```

**树形搜索策略：**
1. **贪心搜索**：每层只保留概率最高的分支
2. **束搜索**：保留top-b个分支
3. **采样搜索**：按概率采样保留分支

**与投机解码的结合：**
多token预测模型可以作为高质量的草稿模型：
```
草稿生成：使用多token预测一次生成k个token
验证阶段：标准目标模型验证
优势：更高的接受率，更少的草稿生成次数
```

**动态策略选择：**
根据上下文动态选择生成策略：
- 高置信度区域：使用更长的多token预测
- 低置信度区域：回退到单token生成
- 特殊token（如标点）：使用专门策略

## 15.3 草稿模型设计与选择

### 15.3.1 草稿模型的设计准则

草稿模型的质量直接影响投机解码的性能。理想的草稿模型应满足：

**速度要求：**
```
τ_q < τ_p / (γ·(1-α))
```
其中τ_q是草稿模型延迟，τ_p是目标模型延迟，γ是草稿长度，α是目标接受率。

**精度要求：**
定义分布距离：
```
D_KL(p||q) = Σ_x p(x) log(p(x)/q(x))
```

理想情况下，D_KL(p||q) → 0。实践中，通常要求：
- 高频token的预测准确率 > 80%
- Top-k (k=10) 覆盖率 > 90%
- 条件熵比率 H(p)/H(q) ≈ 1

**硬件适配考虑：**
1. **内存占用**：草稿模型需常驻内存
   ```
   Memory_q < 0.1 × Memory_p （建议比例）
   ```

2. **计算模式**：适合目标硬件的计算模式
   - GPU：密集矩阵运算
   - CPU：稀疏计算友好
   - DSP/NPU：定点运算

3. **功耗预算**：边缘设备的功耗限制
   ```
   Power_q < 0.2 × Power_p
   ```

### 15.3.2 草稿模型架构选择

**1. 小型语言模型**

架构选择：
- 参数量：目标模型的1/10到1/50
- 层数：4-6层（vs 目标模型24-32层）
- 隐藏维度：512-1024（vs 目标模型4096+）

优化技术：
```
压缩比 = (L_p/L_q) × (d_p/d_q)² × (h_p/h_q)
```
其中L是层数，d是隐藏维度，h是注意力头数。

**2. N-gram模型**

经典但有效的方法，特别适合重复性文本：

实现方式：
```
p_ngram(x_t|x_{t-n+1:t-1}) = count(x_{t-n+1:t}) / count(x_{t-n+1:t-1})
```

优化存储：
- 使用后缀树存储n-gram
- 只保留高频n-gram
- 动态更新统计信息

混合模型：
```
p_hybrid = λ·p_ngram + (1-λ)·p_neural
```

**3. 查找表方法**

对于特定领域或高度结构化的文本：

静态查找：
- 预计算常见prompt的续写
- 使用哈希表快速检索
- 适合模板化生成场景

动态缓存：
```
缓存key: hash(context_last_k_tokens)
缓存value: (next_tokens, probabilities, timestamp)
```

缓存更新策略：
- LRU淘汰
- 基于频率的更新
- 上下文相似度聚类

**4. 神经网络近似**

轻量级架构设计：

a) **深度可分离卷积**：
```
计算量降低比 = (k²·d_in·d_out) / (k²·d_in + d_in·d_out)
```

b) **线性注意力**：
```
Attention(Q,K,V) = φ(Q)(φ(K)ᵀV)
复杂度：O(n·d²) vs O(n²·d)
```

c) **稀疏激活**：
```
使用top-k稀疏化：只激活k个最大的神经元
加速比 ≈ d/k
```

### 15.3.3 自适应草稿模型

**动态模型选择：**

根据输入特征选择最适合的草稿模型：
```
模型池：M = {M_fast, M_balanced, M_accurate}
选择策略：M_t = argmax_m Score(m, context_t)
```

评分函数设计：
```
Score(m, c) = α·Speed(m) + β·Quality(m,c) + γ·Resource(m)
```

**上下文感知调整：**

1. **困惑度引导**：
   ```
   如果 PPL(context) > threshold:
       使用更强的草稿模型
   否则:
       使用更快的草稿模型
   ```

2. **领域检测**：
   - 代码生成：使用专门的代码草稿模型
   - 数学推理：使用保留数学能力的草稿模型
   - 创意写作：使用多样性更高的草稿模型

3. **长度自适应**：
   ```
   γ_adaptive = f(context_length, generation_length)
   ```

**在线学习机制：**

持续优化草稿模型以适应目标分布：

1. **经验回放**：
   ```
   Buffer = [(context, target_tokens, draft_tokens)]
   定期使用Buffer微调草稿模型
   ```

2. **增量学习**：
   ```
   L_online = λ_distill·D_KL(p_target||q_draft) + λ_task·L_original
   ```

3. **元学习框架**：
   ```
   使用MAML等算法快速适应新分布
   θ_adapted = θ - α·∇_θ L(θ, D_new)
   ```

### 15.3.4 草稿模型优化技术

**知识蒸馏：**

温度缩放的软标签蒸馏：
```
L_distill = -Σ_x p_T(x|c) log q_T(x|c)
```
其中p_T和q_T是温度为T的软化分布。

多层蒸馏：
```
L_layer = Σ_l ||f_l^p(x) - g_l^q(x)||²
```
匹配中间层表示。

**参数共享：**

1. **嵌入层共享**：
   ```
   草稿模型和目标模型共享词嵌入
   节省内存：vocab_size × embed_dim
   ```

2. **前缀共享**：
   ```
   草稿模型 = 目标模型前k层 + 轻量级头部
   ```

3. **低秩分解共享**：
   ```
   W_draft = U_shared × V_draft
   W_target = U_shared × V_target
   ```

**量化与剪枝：**

混合精度量化：
```
关键层（如嵌入层）：FP16/BF16
中间层：INT8
非关键层：INT4
```

结构化剪枝策略：
```
1. 通道剪枝：减少隐藏维度
2. 注意力头剪枝：减少多头数量
3. 层剪枝：跳过部分transformer层
```

敏感度分析：
```
S_i = ||∂L/∂W_i||_F / ||W_i||_F
```
优先剪枝敏感度低的部分。

## 15.4 并行解码与批量验证

### 15.4.1 并行解码架构

并行解码的核心思想是同时探索多个可能的生成路径，形成一个解码树或解码图。

**多分支生成：**

树形扩展策略：
```
根节点：初始上下文
第k层：每个节点扩展b个分支
树的宽度：b^k（指数增长）
```

为控制计算复杂度，采用动态剪枝：
```
有效分支数 = min(b^k, B_max)
```

**概率聚合：**
对于路径π = (x_1, x_2, ..., x_n)，其概率为：
```
P(π) = Π_{i=1}^n p(x_i|prefix_i)
```

使用对数概率避免数值下溢：
```
log P(π) = Σ_{i=1}^n log p(x_i|prefix_i)
```

**资源调度算法：**

1. **静态分配**：
   ```
   每个分支分配固定计算资源
   GPU线程数 = 分支数 × 每分支线程数
   ```

2. **动态调度**：
   ```
   优先级 = f(path_probability, path_length, diversity)
   资源分配 ∝ 优先级
   ```

3. **工作窃取**：
   ```
   空闲线程从繁忙线程窃取任务
   适合不平衡的解码树
   ```

### 15.4.2 批量验证机制

**向量化验证实现：**

将多个候选序列组织成批次：
```
输入张量形状：[batch_size, max_seq_len, hidden_dim]
位置编码：[batch_size, max_seq_len]
注意力掩码：[batch_size, max_seq_len, max_seq_len]
```

**批处理策略：**

1. **固定大小批次**：
   ```
   batch = [seq_1, seq_2, ..., seq_B]
   填充到相同长度
   ```

2. **动态批次**：
   ```
   根据序列长度分组
   相似长度的序列放入同一批次
   减少填充开销
   ```

3. **分层批处理**：
   ```
   第1层：验证所有深度为1的候选
   第2层：基于第1层结果，验证深度为2的候选
   ...
   ```

**GPU优化技术：**

1. **融合核函数**：
   ```
   将多个操作融合到单个CUDA kernel
   减少内存访问次数
   ```

2. **张量核心利用**：
   ```
   使用混合精度计算
   FP16计算 + FP32累加
   ```

3. **流水线并行**：
   ```
   Stream 1: 数据传输
   Stream 2: 计算
   Stream 3: 结果回传
   ```

**内存访问模式优化：**

1. **合并访问**：
   ```
   连续线程访问连续内存地址
   提高内存带宽利用率
   ```

2. **共享内存使用**：
   ```
   频繁访问的数据放入共享内存
   如：位置编码、常用激活值
   ```

3. **缓存优化**：
   ```
   L2 cache预取策略
   循环展开减少指令cache miss
   ```

### 15.4.3 动态批处理

**批大小自适应：**

根据系统负载动态调整批大小：
```
B_t = B_{t-1} + α(U_target - U_current)
```
其中U是GPU利用率，α是调整步长。

**延迟-吞吐量平衡：**

定义效用函数：
```
Utility = λ·Throughput - (1-λ)·Latency
```

寻找最优批大小：
```
B_opt = argmax_B Utility(B)
```

实践中的经验公式：
```
B_opt ≈ sqrt(Memory_available / Seq_length)
```

**请求调度策略：**

1. **FIFO（先进先出）**：
   - 简单公平
   - 可能导致队头阻塞

2. **SJF（最短作业优先）**：
   ```
   优先级 = 1 / expected_generation_length
   ```

3. **多级队列**：
   ```
   队列1：短序列（<50 tokens）
   队列2：中等序列（50-200 tokens）
   队列3：长序列（>200 tokens）
   ```

4. **公平排队**：
   ```
   每个用户/会话维护独立队列
   轮询调度保证公平性
   ```

### 15.4.4 系统级优化

**流水线并行：**

将推理过程分解为多个阶段：
```
Stage 1: Prompt编码
Stage 2: 候选生成
Stage 3: 批量验证
Stage 4: 结果聚合
```

流水线效率：
```
Efficiency = (n·t_max) / (n·t_max + (k-1)·t_max)
```
其中n是批次数，k是流水线阶段数，t_max是最慢阶段的时间。

**异步执行模式：**

1. **生产者-消费者模式**：
   ```
   生产者：生成候选序列
   消费者：验证候选序列
   通过队列解耦
   ```

2. **Future/Promise模式**：
   ```
   异步提交验证任务
   继续生成新候选
   需要时获取结果
   ```

3. **协程模式**：
   ```
   使用协程实现轻量级并发
   适合I/O密集型操作
   ```

**缓存策略设计：**

1. **KV Cache分层**：
   ```
   L1: GPU HBM（高频访问）
   L2: GPU显存（中频访问）
   L3: 系统内存（低频访问）
   ```

2. **预测性预取**：
   ```
   基于访问模式预测未来需求
   提前将数据移动到高速缓存
   ```

3. **缓存压缩**：
   ```
   使用量化压缩KV Cache
   INT8量化可减少50%内存占用
   ```

4. **智能淘汰策略**：
   ```
   综合考虑：
   - 访问频率
   - 访问时间
   - 重计算成本
   - 内存压力
   ```

**边缘设备特殊优化：**

1. **功耗感知调度**：
   ```
   低功耗模式：降低并行度
   高性能模式：最大化并行
   自适应切换
   ```

2. **热管理**：
   ```
   监控芯片温度
   动态调整计算强度
   避免热节流
   ```

3. **内存受限优化**：
   ```
   激进的内存复用
   计算换内存策略
   增量式计算
   ```

## 本章小结

本章深入探讨了大语言模型解码加速的核心技术。投机解码通过引入草稿模型实现了推理加速，在保持输出分布不变的同时，将串行生成过程部分并行化。我们推导了投机解码的数学框架，包括接受-拒绝采样机制和最优草稿长度的理论分析。多token预测技术从根本上改变了模型的训练和推理范式，通过联合预测多个未来token提高了生成效率。在草稿模型设计方面，我们讨论了从简单的n-gram模型到复杂的神经网络近似的多种选择，以及知识蒸馏、参数共享等优化技术。最后，我们详细分析了并行解码架构和批量验证机制，包括GPU优化、动态批处理和系统级的流水线设计。

关键要点：
1. 投机解码的加速比取决于草稿模型质量（接受率α）和速度比（τ_p/τ_q）
2. 多token预测通过联合建模提供了信息论上的优势
3. 草稿模型设计需要在速度、精度和资源消耗间权衡
4. 批量验证和并行解码可以充分利用硬件并行性
5. 边缘设备需要特殊的功耗和内存优化策略

## 练习题

### 基础题

1. **投机解码分析**
   假设目标模型推理时间τ_p=100ms，草稿模型推理时间τ_q=10ms，平均接受率α=0.8。计算当草稿长度γ=4时的理论加速比。

   *Hint: 使用加速比公式 Speedup = [α(1-α^γ)/(1-α)]·τ_p / (γ·τ_q + τ_p)*

   <details>
   <summary>答案</summary>
   
   E[n_accept] = 0.8(1-0.8^4)/(1-0.8) = 0.8(1-0.4096)/0.2 = 2.36
   
   T_spec = 4×10 + 100 = 140ms
   
   T_standard = 2.36×100 = 236ms
   
   Speedup = 236/140 ≈ 1.69倍
   </details>

2. **KL散度计算**
   给定目标分布p=[0.5, 0.3, 0.2]和草稿分布q=[0.6, 0.3, 0.1]，计算KL散度D_KL(p||q)。

   *Hint: D_KL(p||q) = Σ p(x)log(p(x)/q(x))*

   <details>
   <summary>答案</summary>
   
   D_KL(p||q) = 0.5×log(0.5/0.6) + 0.3×log(0.3/0.3) + 0.2×log(0.2/0.1)
             = 0.5×(-0.182) + 0.3×0 + 0.2×0.693
             = -0.091 + 0 + 0.139
             = 0.048
   </details>

3. **批处理效率**
   如果单个序列推理需要50MB显存，GPU总显存为8GB，考虑到系统开销后可用显存为6GB。在序列长度为512的情况下，计算最优批大小。

   *Hint: 使用经验公式 B_opt ≈ sqrt(Memory_available / Seq_length)*

   <details>
   <summary>答案</summary>
   
   首先计算显存约束下的最大批大小：
   B_max = 6000MB / 50MB = 120
   
   使用经验公式：
   B_opt ≈ sqrt(6000MB / 512) ≈ sqrt(11.7) ≈ 3.4
   
   考虑到显存约束和经验公式，实际可选择批大小为4-8之间。
   </details>

### 挑战题

4. **最优草稿长度推导**
   推导在给定硬件条件下的最优草稿长度公式。假设接受率随位置指数衰减：α_i = α_0 × β^(i-1)，其中β < 1。

   *Hint: 构建期望加速比函数并求导*

   <details>
   <summary>答案</summary>
   
   期望接受token数：
   E[n] = Σ_{i=1}^γ α_0β^(i-1) = α_0(1-β^γ)/(1-β)
   
   加速比函数：
   S(γ) = [α_0(1-β^γ)/(1-β)]τ_p / (γτ_q + τ_p)
   
   对γ求导并令其为0，得到：
   ∂S/∂γ = 0
   
   经过推导可得最优草稿长度满足：
   β^γ[γlog(β) - 1] + 1 = (τ_q/τ_p)(1-β)/α_0
   
   这是一个超越方程，需要数值求解。
   </details>

5. **多Token预测的信息增益**
   证明：对于具有马尔可夫性质的序列，k步联合预测相比独立预测的信息增益上界为(k-1)H(X)，其中H(X)是单token熵。

   *Hint: 使用条件熵的链式法则*

   <details>
   <summary>答案</summary>
   
   根据链式法则：
   H(X_1,...,X_k|X_0) = Σ_{i=1}^k H(X_i|X_0,...,X_{i-1})
   
   对于马尔可夫链：
   H(X_i|X_0,...,X_{i-1}) = H(X_i|X_{i-1}) = H(X)
   
   因此联合熵：
   H(X_1,...,X_k|X_0) ≤ k·H(X)
   
   独立预测的熵和：
   Σ_{i=1}^k H(X_i|X_0) = k·H(X|X_0)
   
   信息增益 = 独立预测熵 - 联合预测熵 ≤ (k-1)H(X)
   </details>

6. **动态批处理算法设计**
   设计一个动态批处理算法，目标是在满足延迟SLA（Service Level Agreement）的前提下最大化吞吐量。给定：每个请求的最大允许延迟为D_max，当前队列中有N个请求，预测每个请求需要生成L_i个token。

   *Hint: 考虑使用动态规划或贪心算法*

   <details>
   <summary>答案</summary>
   
   算法设计：
   
   1. 将请求按预测长度L_i排序
   2. 初始化批次B = ∅
   3. 对每个请求i：
      - 预测加入B后的完成时间T_B∪{i}
      - 如果T_B∪{i} ≤ D_max - T_elapsed，将i加入B
      - 否则，执行当前批次B，开始新批次
   
   时间预测模型：
   T_batch = T_encode + max_i(L_i) × T_decode_step
   
   其中T_encode ∝ |B|（批大小），T_decode_step相对固定。
   
   优化目标：
   maximize Σ|B_i| / Σ T_batch_i
   subject to: ∀请求j, T_complete_j ≤ T_arrive_j + D_max
   </details>

7. **草稿模型在线适应**（开放题）
   设计一个在线学习算法，使草稿模型能够根据用户的使用模式动态调整。考虑以下因素：
   - 不同domain的文本特征差异
   - 计算和存储资源限制
   - 隐私保护要求

   *Hint: 可以考虑联邦学习、增量学习或元学习方法*

   <details>
   <summary>参考思路</summary>
   
   可能的方案：
   
   1. **增量微调方案**：
      - 维护一个小的经验缓冲区
      - 定期使用LoRA等参数高效方法微调
      - 使用EWC（Elastic Weight Consolidation）避免灾难性遗忘
   
   2. **动态词表适应**：
      - 统计用户高频词汇
      - 动态调整词表和嵌入
      - 使用布隆过滤器减少存储
   
   3. **混合专家架构**：
      - 预训练多个domain专家
      - 根据输入特征动态路由
      - 在线更新路由权重
   
   4. **隐私保护机制**：
      - 差分隐私训练
      - 本地计算，只上传梯度
      - 安全聚合协议
   </details>

8. **系统瓶颈分析**（开放题）
   给定一个边缘设备的硬件规格（如：4GB内存、4核ARM CPU、峰值功耗10W），分析在部署7B参数模型时的系统瓶颈，并提出综合优化方案。

   *Hint: 考虑计算、内存、带宽和功耗等多个维度*

   <details>
   <summary>参考分析</summary>
   
   瓶颈分析：
   
   1. **内存瓶颈**：
      - 7B FP16参数需要14GB
      - 解决：4-bit量化降至3.5GB
      - KV Cache额外需求：~500MB @seq_len=2048
   
   2. **计算瓶颈**：
      - 单token推理：~14 GFLOPs
      - ARM CPU: ~50 GFLOPs (理论峰值)
      - 实际利用率：~20%
   
   3. **功耗瓶颈**：
      - 持续高负载超过功耗预算
      - 需要动态频率调整
   
   综合优化方案：
   - 使用INT4量化 + 稀疏化
   - 投机解码with 0.5B草稿模型
   - 动态batch和功耗感知调度
   - 选择性卸载到云端
   </details>