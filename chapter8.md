# 第8章：量化工具链

在前面的章节中，我们深入探讨了各种量化算法的理论基础。本章将聚焦于实际的量化工具链，介绍如何将这些理论应用到边缘部署场景中。我们将详细分析主流量化工具的设计思想、实现原理和最佳实践，帮助读者构建高效的边缘推理系统。

## 8.1 Bitsandbytes：实用量化库

### 8.1.1 设计理念与架构

Bitsandbytes是由华盛顿大学Tim Dettmers团队开发的量化库，其核心设计理念是在保持模型精度的同时，最大限度地减少内存占用和计算开销。该库的架构特点包括：

1. **分块量化策略**：将权重矩阵分成小块进行独立量化，每块维护自己的量化参数
2. **动态量化范围**：根据数据分布自适应调整量化范围
3. **异常值特殊处理**：识别并单独存储超出正常范围的权重值
4. **高效CUDA实现**：针对GPU架构优化的量化/反量化kernel

### 8.1.2 8-bit量化算法：Linear8bitLt

Linear8bitLt是Bitsandbytes的核心8-bit量化算法，其数学原理如下：

**量化过程**：
对于权重矩阵 W ∈ ℝ^(m×n)，首先将其划分为大小为 B 的块：

W = [W₁, W₂, ..., Wₖ]，其中 k = ⌈n/B⌉

对每个块 Wᵢ，计算其最大绝对值：
```
αᵢ = max(|Wᵢ|)
```

量化公式：
```
W̃ᵢ = round(127 × Wᵢ / αᵢ)
```

**异常值处理**：
识别异常值的准则：
```
|wᵢⱼ| > 6σ，其中 σ 是权重的标准差
```

异常值以FP16格式单独存储，形成稀疏矩阵 S。

**反量化计算**：
```
ŷ = (W̃ × x) ⊙ (α/127) + S × x
```

其中 ⊙ 表示逐元素乘法，α 是缩放因子向量。

### 8.1.3 4-bit量化：NF4与QLoRA

**NF4（Normal Float 4-bit）量化**：

NF4是专门为正态分布数据设计的4-bit数据类型。其量化级别通过以下优化问题确定：

```
Q* = argmin_Q E_X~N(0,1)[||X - Q(X)||²]
```

量化级别的计算步骤：
1. 将标准正态分布均匀划分为16个区间
2. 每个区间的代表值取该区间的期望值
3. 生成的16个量化级别关于0对称

**QLoRA优化**：

QLoRA在NF4基础上引入了以下优化：

1. **双重量化**：对量化常数本身进行二次量化
   ```
   c₂ = quantize(c₁, bits=8)
   ```

2. **分页优化器状态**：将优化器状态分页存储，按需加载
   ```
   内存占用：4-bit权重 + 16-bit梯度 + 分页的32-bit优化器状态
   ```

3. **LoRA适配器的梯度计算**：
   ```
   ∇L/∇A = (∇L/∇Y) × Bᵀ × xᵀ
   ∇L/∇B = Aᵀ × (∇L/∇Y) × xᵀ
   ```

### 8.1.4 动态量化与混合精度策略

**动态量化算法**：

对于激活值的动态量化，Bitsandbytes采用percentile量化：

```
α = percentile(|X|, p=99.9)
X̃ = clip(round(X × 127/α), -128, 127)
```

**混合精度决策**：

基于层敏感度的混合精度分配：
```
sensitivity(l) = ||W_l^FP16 - W_l^INT8||_F / ||W_l^FP16||_F
```

根据敏感度阈值τ决定精度：
```
precision(l) = {
    FP16, if sensitivity(l) > τ
    INT8, otherwise
}
```

### 8.1.5 与主流框架的集成

Bitsandbytes通过以下机制实现与PyTorch、Transformers等框架的无缝集成：

1. **自定义Linear层替换**：
   ```
   替换规则：nn.Linear → bnb.nn.Linear8bitLt
   ```

2. **自动混合精度兼容**：
   与PyTorch AMP协同工作，自动处理精度转换

3. **梯度检查点优化**：
   量化权重在前向传播时动态反量化，减少内存占用

## 8.2 GGUF格式与llama.cpp

### 8.2.1 GGUF格式设计原理

GGUF（GPT-Generated Unified Format）是llama.cpp项目采用的新一代模型格式，相比之前的GGML格式有以下改进：

**文件结构**：
```
GGUF文件 = Header + KV元数据 + 张量信息 + 对齐填充 + 张量数据
```

**Header设计**：
- Magic Number: "GGUF" (0x46554747)
- Version: 当前为3
- Tensor Count: 张量总数
- KV Pair Count: 元数据键值对数量

**元数据存储**：
支持的数据类型包括：
- 标量类型：uint8到float64
- 字符串类型：UTF-8编码
- 数组类型：同类型元素数组

**张量布局优化**：
- 内存对齐：32字节边界对齐
- 连续存储：相关张量物理相邻
- 压缩编码：支持多种量化格式

### 8.2.2 量化方案详解：Q4_0到Q8_0

**Q4_0量化（4-bit，32个元素一组）**：

量化过程：
```
对于32个FP16值的块 x[0..31]：
1. 计算缩放因子：d = max(|x[i]|) / 7
2. 量化：q[i] = round(x[i] / d)，范围[-8, 7]
3. 存储：每个q[i]占4 bits
```

存储格式（每块18字节）：
```
[d:FP16][q[0]:4bit][q[1]:4bit]...[q[31]:4bit]
```

**Q4_1量化（4-bit + 最小值）**：

改进：增加最小值偏移
```
d = (max(x) - min(x)) / 15
m = min(x)
q[i] = round((x[i] - m) / d)，范围[0, 15]
```

**Q5_0和Q5_1量化**：

使用5 bits表示，提供更高精度：
- Q5_0: 有符号，范围[-16, 15]
- Q5_1: 无符号 + 偏移，范围[0, 31]

**Q8_0量化**：

最高精度的量化格式：
```
d = max(|x[i]|) / 127
q[i] = round(x[i] / d)，范围[-128, 127]
```

### 8.2.3 K-quants系列优化

K-quants是llama.cpp引入的新一代量化方案，主要优化包括：

**超级块结构**：
将256个元素组成一个超级块，内部分为多个子块：
```
K-quants超级块 = 全局缩放 + 子块缩放数组 + 量化数据
```

**重要性加权量化**：
```
对于权重w和重要性i：
q = round(w × scale × sqrt(i))
```

**混合精度子块**：
- 高重要性子块：6-bit量化
- 低重要性子块：4-bit量化
- 自适应分配比例

**K-quants变体**：
- K_S (Small): 优化模型大小
- K_M (Medium): 平衡大小和质量
- K_L (Large): 优化质量

### 8.2.4 重要性矩阵（imatrix）量化

**重要性矩阵计算**：

通过在校准数据集上运行模型，收集激活统计：
```
I[i,j] = E[|x[i] × w[j]|]
```

其中x是输入激活，w是权重。

**基于重要性的量化**：

1. **重要性分组**：
   ```
   将权重按重要性排序，分为K组
   每组使用不同的量化比特数
   ```

2. **自适应缩放**：
   ```
   scale[k] = f(importance[k]) × base_scale
   ```

3. **误差最小化**：
   ```
   min Σᵢⱼ I[i,j] × (w[i,j] - q[i,j])²
   ```

### 8.2.5 边缘部署最佳实践

**内存映射优化**：
```
使用mmap直接映射GGUF文件
按需加载张量数据
支持部分模型加载
```

**CPU优化策略**：
1. SIMD指令集使用（AVX2/AVX512/NEON）
2. 缓存友好的数据布局
3. 多线程并行计算

**量化格式选择指南**：
```
移动设备（<4GB）: Q4_0或Q4_K_S
嵌入式设备（4-8GB）: Q5_K_M或Q4_K_M  
桌面设备（>8GB）: Q5_K_L或Q6_K
```

**性能调优建议**：
1. 批处理大小优化：根据缓存大小调整
2. 线程数配置：通常设为物理核心数
3. NUMA感知：大型服务器上的内存亲和性设置

## 8.3 量化感知训练（QAT）实践

### 8.3.1 QAT vs PTQ的权衡

量化感知训练（QAT）和训练后量化（PTQ）各有优劣，选择合适的方案需要考虑多个因素：

**精度-效率权衡分析**：

| 维度 | PTQ | QAT |
|------|-----|-----|
| 精度损失 | 1-3% (INT8), 3-10% (INT4) | <1% (INT8), 1-3% (INT4) |
| 训练成本 | 无需重训练 | 需要10-20%原始训练时间 |
| 数据需求 | 少量校准数据 | 完整训练数据集 |
| 实施复杂度 | 低 | 高 |

**QAT的优势场景**：
1. 极低比特量化（INT4及以下）
2. 对精度要求极高的应用
3. 模型结构对量化敏感
4. 有充足的训练资源

**数学原理对比**：

PTQ优化目标：
```
min ||W - Q(W)||²_F
```

QAT优化目标：
```
min E[L(f_Q(x; W), y)]
```

其中f_Q表示量化模型，L是任务损失函数。

### 8.3.2 伪量化与梯度传播

**伪量化（Fake Quantization）机制**：

前向传播：
```
x_q = clamp(round(x/s), q_min, q_max) × s
```

其中s是缩放因子，[q_min, q_max]是量化范围。

**直通估计器（STE）**：

由于round函数不可导，使用STE近似梯度：
```
∂L/∂x = ∂L/∂x_q × 1_{x∈[α,β]}
```

其中1_{x∈[α,β]}是指示函数，[α, β]是量化范围。

**改进的梯度估计**：

1. **软量化函数**：
   ```
   x_q = s × tanh(x/s × k) × (q_max - q_min)/2
   ```
   其中k控制软化程度。

2. **学习型梯度**：
   ```
   ∂round(x)/∂x ≈ σ(p₁(x - ⌊x⌋ - 0.5))
   ```
   其中σ是sigmoid函数，p₁是可学习参数。

### 8.3.3 LSQ（Learned Step-size Quantization）

LSQ是目前最有效的QAT方法之一，其核心创新是将量化步长作为可学习参数。

**量化公式**：
```
w_q = s × clamp(round(w/s), -2^(b-1), 2^(b-1)-1)
```

**步长梯度计算**：

对于权重量化：
```
∂L/∂s_w = ∂L/∂w_q × ∂w_q/∂s_w
```

其中：
```
∂w_q/∂s_w = {
    -w/s + ⌊w/s⌋, if -Q_N < w/s < Q_P
    -Q_N, if w/s ≤ -Q_N
    Q_P, if w/s ≥ Q_P
}
```

**步长初始化策略**：
```
s_init = 2×mean(|w|) / √(Q_P)
```

**梯度缩放**：
为平衡步长和权重的梯度量级：
```
g_s = g_s × √(n) / ||g_w||₂
```

其中n是权重数量。

### 8.3.4 渐进式量化训练策略

**比特数渐进**：
```
训练阶段：FP32 → INT16 → INT8 → INT4
每阶段训练：原始epoch数的20%
```

**量化层渐进**：
1. 第一阶段：仅量化权重
2. 第二阶段：加入激活量化
3. 第三阶段：全模型量化

**温度退火策略**：

使用Gumbel Softmax进行软量化：
```
q_soft = Σᵢ exp((log(πᵢ) + gᵢ)/τ) × vᵢ
```

温度τ的退火计划：
```
τ(t) = τ₀ × exp(-λt)
```

**知识蒸馏增强**：

结合QAT和知识蒸馏：
```
L_total = L_task + α × L_KD
L_KD = KL(p_teacher || p_student)
```

其中α随训练进程递减。

### 8.3.5 大模型QAT的挑战与解决方案

**内存挑战**：

QAT需要存储：
- FP32主权重
- 量化权重
- 梯度
- 优化器状态

解决方案：

1. **梯度检查点**：
   ```
   仅保存关键激活，需要时重计算
   内存节省：O(√n) vs O(n)
   ```

2. **混合精度QAT**：
   ```
   主权重：FP16
   梯度累积：FP32
   量化权重：INT8/INT4
   ```

3. **层冻结策略**：
   ```
   冻结已收敛层，仅训练敏感层
   内存减少：~60%
   ```

**收敛性挑战**：

1. **学习率调整**：
   ```
   lr_qat = lr_pretrain × 0.1
   使用余弦退火而非阶梯下降
   ```

2. **批归一化校准**：
   ```
   QAT后需要重新估计BN统计量
   使用代表性数据运行10-50个batch
   ```

3. **量化感知正则化**：
   ```
   R_quant = λ × Σᵢ (range(wᵢ))²
   ```
   防止权重分布过宽。

**分布式QAT**：

1. **数据并行**：
   每个GPU维护完整模型副本
   
2. **模型并行**：
   按层划分，减少单GPU内存需求

3. **量化参数同步**：
   ```
   AllReduce(scales, min_values, max_values)
   ```

## 8.4 量化误差分析与补偿

### 8.4.1 量化误差的来源与传播

**误差来源分析**：

1. **舍入误差**：
   ```
   ε_round = w - Q(w) = w - s × round(w/s)
   |ε_round| ≤ s/2
   ```

2. **饱和误差**：
   ```
   ε_clip = {
       w - s × q_max, if w > s × q_max
       w - s × q_min, if w < s × q_min
       0, otherwise
   }
   ```

3. **量化步长误差**：
   非最优步长导致的额外误差：
   ```
   ε_scale = ||W - Q(W; s)||² - ||W - Q(W; s*)||²
   ```
   其中s*是最优步长。

**误差传播分析**：

对于深度网络，第l层的误差会传播到输出：
```
ε_out^(l) = ∏ᵢ₌ₗ₊₁ᴸ W⁽ⁱ⁾ × ε^(l)
```

误差放大因子：
```
κ = ||∏ᵢ₌ₗ₊₁ᴸ W⁽ⁱ⁾||₂
```

**累积误差估计**：

假设各层误差独立，总误差的期望和方差：
```
E[ε_total] = Σₗ E[ε^(l)]
Var[ε_total] = Σₗ Var[ε^(l)] × κₗ²
```

### 8.4.2 误差度量方法：MSE、余弦相似度、KL散度

**均方误差（MSE）**：
```
MSE = 1/n × ||W - W_q||²_F = 1/n × Σᵢⱼ(wᵢⱼ - w_q_ij)²
```

优点：直观，易于优化
缺点：对异常值敏感

**相对误差**：
```
RE = ||W - W_q||_F / ||W||_F
```

**余弦相似度**：
```
cos_sim = (W · W_q) / (||W||₂ × ||W_q||₂)
```

用于衡量方向保持程度，对缩放不敏感。

**KL散度（用于输出分布）**：
```
KL(P||Q) = Σᵢ p_i × log(p_i/q_i)
```

其中P是原始模型输出分布，Q是量化模型输出。

**层级误差度量**：

对于激活值分布：
```
JS(A||A_q) = 1/2 × KL(A||M) + 1/2 × KL(A_q||M)
```
其中M = (A + A_q)/2，JS散度对称且有界。

**任务相关度量**：

1. **困惑度变化**（语言模型）：
   ```
   ΔPPL = PPL_quant - PPL_float
   ```

2. **准确率下降**（分类任务）：
   ```
   ΔAcc = Acc_float - Acc_quant
   ```

### 8.4.3 误差补偿技术：偏置校正、尺度调整

**偏置校正**：

量化引入的系统性偏差：
```
bias = E[W_q - W] = E[ε]
```

校正方法：
```
W_q_corrected = W_q - bias
```

**通道级尺度调整**：

为每个输出通道优化缩放因子：
```
s_c* = argmin_s ||W_c - Q(W_c; s)||²
```

**激活值范围校准**：

使用批归一化统计进行校准：
```
x_calibrated = (x - μ_q) × σ_f/σ_q + μ_f
```

其中μ_f, σ_f是浮点模型统计，μ_q, σ_q是量化模型统计。

**误差注入训练**：

在训练时模拟量化误差：
```
W_train = W + η × ε_simulated
ε_simulated ~ N(0, σ²_quant)
```

### 8.4.4 敏感层识别与混合精度分配

**基于Hessian的敏感度分析**：

层敏感度定义：
```
S_l = Tr(H_l × Σ_ε)
```

其中H_l是损失函数关于第l层权重的Hessian矩阵，Σ_ε是量化误差协方差。

**一阶近似方法**：
```
S_l ≈ ||∇_W L||₂ × ||ε||₂
```

**基于泰勒展开的分析**：

损失变化的二阶近似：
```
ΔL ≈ ε^T × g + 1/2 × ε^T × H × ε
```

**动态敏感度评估**：

在不同输入batch上评估：
```
S_l_dynamic = 1/B × Σᵦ ||f_l(x_b) - f_l_q(x_b)||₂
```

**混合精度分配算法**：

1. **贪心分配**：
   ```
   while memory_used < budget:
       l* = argmax_l S_l / cost_l
       precision[l*] += 1
       update S_l*
   ```

2. **动态规划**：
   ```
   dp[i][m] = min accuracy loss using m bits for layers 1..i
   ```

3. **强化学习方法**：
   使用策略网络学习精度分配策略。

### 8.4.5 端到端精度评估框架

**评估流程设计**：

1. **数据集准备**：
   - 训练集子集（校准用）
   - 验证集（评估用）
   - 对抗样本（鲁棒性测试）

2. **多维度评估**：

```
评估指标体系：
├── 精度指标
│   ├── 任务指标（准确率/BLEU/困惑度）
│   ├── 层级MSE
│   └── 输出分布相似度
├── 性能指标
│   ├── 延迟（首token/总体）
│   ├── 吞吐量
│   └── 内存占用
└── 鲁棒性指标
    ├── 对抗样本表现
    └── 分布偏移适应性
```

**自动化评估工具**：

1. **精度扫描**：
   ```
   for bit in [2, 4, 6, 8]:
       for method in [symmetric, asymmetric]:
           evaluate_model(bit, method)
   ```

2. **性能profiling**：
   ```
   记录每层的：
   - 计算时间
   - 内存访问模式
   - Cache命中率
   ```

**误差溯源分析**：

1. **逐层误差分解**：
   ```
   ε_total = Σₗ contribution_l
   contribution_l = ∂L/∂f_l × ε_l
   ```

2. **关键路径识别**：
   找出对最终输出影响最大的层序列。

**评估报告生成**：

自动生成包含以下内容的报告：
- 精度-压缩率曲线
- 层级敏感度热图
- 性能瓶颈分析
- 优化建议

## 本章小结

本章系统介绍了量化工具链的核心组件和实践方法。我们深入分析了：

1. **Bitsandbytes库**的设计理念和算法实现，包括8-bit的Linear8bitLt算法和4-bit的NF4量化，以及QLoRA优化技术。这些方法通过分块量化、异常值处理和双重量化等技术，实现了高效的模型压缩。

2. **GGUF格式和llama.cpp**的量化方案，从Q4_0到Q8_0的各种量化格式，以及K-quants系列的优化。重要性矩阵（imatrix）量化技术能够根据权重的实际重要性分配不同的量化精度。

3. **量化感知训练（QAT）**的实践方法，包括伪量化机制、LSQ算法、渐进式训练策略等。我们讨论了大模型QAT面临的内存和收敛性挑战，以及相应的解决方案。

4. **量化误差分析与补偿**技术，涵盖误差来源、传播机制、度量方法和补偿策略。通过敏感层识别和混合精度分配，可以在有限的比特预算下获得最佳的模型性能。

关键公式回顾：
- 量化基本公式：`W̃ = round(W × scale) / scale`
- LSQ步长梯度：`∂L/∂s = ∂L/∂w_q × ∂w_q/∂s`
- 误差传播：`ε_out = ∏W × ε_in`
- 层敏感度：`S_l = Tr(H_l × Σ_ε)`

## 练习题

### 基础题

1. **Bitsandbytes异常值处理**
   
   给定一个权重向量W = [0.1, 0.2, 15.5, 0.3, -0.1, 0.4]，标准差σ = 0.15，使用6σ准则识别异常值。计算将正常值量化为INT8后的结果。
   
   *Hint*: 先计算6σ的阈值，识别超出范围的值，然后对剩余值进行量化。

   <details>
   <summary>答案</summary>
   
   6σ = 6 × 0.15 = 0.9
   异常值：15.5（|15.5| > 0.9）
   正常值最大绝对值：0.4
   缩放因子：α = 0.4
   量化结果：[32, 63, FP16(15.5), 95, -32, 127]
   </details>

2. **GGUF Q4_0量化计算**
   
   对以下32个FP16值进行Q4_0量化，计算缩放因子d和量化后的值（只计算前4个）：
   [0.5, -0.3, 0.8, -0.6, ...]，假设最大绝对值为0.8。
   
   *Hint*: Q4_0的量化范围是[-8, 7]，需要先计算缩放因子。

   <details>
   <summary>答案</summary>
   
   d = max(|x|) / 7 = 0.8 / 7 ≈ 0.114
   量化值：
   - 0.5 / 0.114 ≈ 4.4 → 4
   - -0.3 / 0.114 ≈ -2.6 → -3
   - 0.8 / 0.114 ≈ 7.0 → 7
   - -0.6 / 0.114 ≈ -5.3 → -5
   </details>

3. **QAT中的STE梯度计算**
   
   在量化感知训练中，如果量化函数输出x_q = 3.0，原始值x = 3.4，量化范围[0, 10]，上游梯度∂L/∂x_q = 0.5，使用直通估计器计算∂L/∂x。
   
   *Hint*: STE假设在量化范围内梯度直接传递。

   <details>
   <summary>答案</summary>
   
   因为x = 3.4在量化范围[0, 10]内，根据STE：
   ∂L/∂x = ∂L/∂x_q × 1 = 0.5
   </details>

4. **量化误差的MSE计算**
   
   原始权重矩阵W = [[1.0, 2.0], [3.0, 4.0]]，量化后W_q = [[1.1, 1.9], [3.2, 3.8]]，计算MSE。
   
   *Hint*: MSE = 平均平方误差

   <details>
   <summary>答案</summary>
   
   误差矩阵：[[0.1, -0.1], [0.2, -0.2]]
   MSE = (0.1² + 0.1² + 0.2² + 0.2²) / 4 = 0.1 / 4 = 0.025
   </details>

### 挑战题

5. **K-quants超级块设计**
   
   设计一个256元素的K-quants超级块结构，其中30%的子块使用6-bit量化（高重要性），70%使用4-bit量化。计算总的存储开销（bits）。假设全局缩放因子16-bit，每个子块缩放因子8-bit，子块大小为32。
   
   *Hint*: 需要计算子块数量、不同精度的存储需求和元数据开销。

   <details>
   <summary>答案</summary>
   
   子块数：256 / 32 = 8个
   6-bit子块：8 × 0.3 ≈ 2个（实际取2）
   4-bit子块：8 - 2 = 6个
   
   存储计算：
   - 全局缩放：16 bits
   - 子块缩放：8 × 8 = 64 bits
   - 6-bit数据：2 × 32 × 6 = 384 bits
   - 4-bit数据：6 × 32 × 4 = 768 bits
   - 总计：16 + 64 + 384 + 768 = 1232 bits
   
   压缩率：1232 / (256 × 16) ≈ 30%
   </details>

6. **LSQ步长优化问题**
   
   给定一组权重w = [1.2, -0.8, 2.1, -1.5]，使用3-bit量化（范围[-4, 3]），计算最优初始步长s_init。如果当前损失梯度∂L/∂w_q = [0.1, -0.2, 0.3, -0.1]，推导∂L/∂s的值。
   
   *Hint*: 使用LSQ的步长初始化公式和梯度计算公式。

   <details>
   <summary>答案</summary>
   
   初始步长计算：
   mean(|w|) = (1.2 + 0.8 + 2.1 + 1.5) / 4 = 1.4
   Q_P = 3
   s_init = 2 × 1.4 / √3 ≈ 1.62
   
   梯度计算需要考虑每个权重的量化状态，这是一个复杂的分段函数。
   </details>

7. **混合精度分配的动态规划**
   
   有3层网络，每层可选择4-bit或8-bit量化。各层的精度损失和内存占用如下表。总内存预算为150MB，求最小精度损失的分配方案。
   
   | 层 | 4-bit损失 | 4-bit内存 | 8-bit损失 | 8-bit内存 |
   |----|-----------|-----------|-----------|-----------|
   | 1  | 0.05      | 40MB      | 0.01      | 80MB      |
   | 2  | 0.08      | 30MB      | 0.02      | 60MB      |
   | 3  | 0.03      | 20MB      | 0.005     | 40MB      |
   
   *Hint*: 定义dp[i][m]为前i层使用m内存的最小损失。

   <details>
   <summary>答案</summary>
   
   使用动态规划求解：
   最优方案：层1用8-bit，层2用4-bit，层3用8-bit
   总内存：80 + 30 + 40 = 150MB
   总损失：0.01 + 0.08 + 0.005 = 0.095
   </details>

8. **量化误差传播分析**
   
   考虑一个3层网络，每层的权重矩阵范数||W||₂分别为[2.0, 1.5, 1.2]，每层的量化误差标准差为[0.01, 0.02, 0.015]。假设误差独立，计算第1层误差传播到输出的放大因子，以及总输出误差的标准差。
   
   *Hint*: 误差通过后续层的权重矩阵传播，使用误差传播公式。

   <details>
   <summary>答案</summary>
   
   第1层误差的放大因子：
   κ₁ = ||W₂|| × ||W₃|| = 1.5 × 1.2 = 1.8
   
   各层误差对输出的贡献：
   - 层1：Var₁ = 0.01² × 1.8² = 0.000324
   - 层2：Var₂ = 0.02² × 1.2² = 0.000576
   - 层3：Var₃ = 0.015² × 1² = 0.000225
   
   总方差：Var_total = 0.000324 + 0.000576 + 0.000225 = 0.001125
   标准差：σ_total = √0.001125 ≈ 0.0335
   </details>