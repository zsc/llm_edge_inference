# 第10章：稀疏化与参数共享

稀疏化和参数共享是模型压缩领域的两大核心技术。稀疏化通过将部分权重置零来减少计算量，而参数共享则通过复用权重来降低模型参数量。本章将深入探讨结构化稀疏、模型合并等前沿技术，重点关注它们在边缘推理场景中的实际应用。我们将从数学原理出发，详细分析各种方法的设计思想、硬件适配性以及实际部署中的权衡。

## 10.1 2:4结构化稀疏

### 10.1.1 基本概念与定义

2:4结构化稀疏是NVIDIA在Ampere架构中引入的细粒度结构化剪枝技术。其核心约束是：在每4个连续权重中，必须有且仅有2个权重为零。

对于权重矩阵 W ∈ ℝ^(m×n)，我们将其按行展开，每4个元素为一组：
- 组 G_i = [w_{i,4j}, w_{i,4j+1}, w_{i,4j+2}, w_{i,4j+3}]
- 约束：||G_i||_0 = 2，即每组恰好有2个非零元素

这种模式实现了固定的50%稀疏率，同时保持了硬件友好的规则性。

### 10.1.2 数学原理与优化目标

2:4稀疏化过程可以形式化为一个组合优化问题。对于每个4元素组，我们需要选择保留哪2个权重：

**优化目标：**
$$\min_{\mathcal{M}} \mathcal{L}(f(X; W \odot \mathcal{M}))$$

其中：
- $\mathcal{M}$ 是稀疏掩码，$\mathcal{M}_{ij} \in \{0,1\}$
- $\odot$ 表示逐元素乘积
- $\mathcal{L}$ 是损失函数
- 约束：每4个连续元素中恰好2个为1

**贪心近似算法：**
实践中采用基于幅值的贪心策略：
1. 对每组 G_i，计算权重幅值：|w_{i,j}|
2. 保留幅值最大的2个权重，其余置零
3. 生成稀疏掩码：$\mathcal{M}_{i,j} = \mathbb{1}[w_{i,j} \in \text{Top-2}(G_i)]$

### 10.1.3 硬件加速机制

NVIDIA Ampere架构引入了专门的稀疏张量核心（Sparse Tensor Cores），实现了2:4稀疏的硬件级加速。

**压缩存储格式：**
- 非零值数组：存储2个非零权重值
- 索引元数据：3比特编码6种可能的位置组合
- 存储效率：相比稠密存储减少50%

**计算流程：**
1. **数据加载**：从内存读取压缩的权重和索引
2. **动态路由**：硬件根据索引将非零权重路由到计算单元
3. **跳过零计算**：完全避免零值乘法操作
4. **吞吐量提升**：理论上可达2倍，实际1.3-1.7倍

### 10.1.4 精度恢复策略

直接剪枝会导致精度下降，需要通过微调恢复：

**渐进式稀疏化：**
$$W^{(t+1)} = W^{(t)} - \eta \nabla_W \mathcal{L} \cdot \mathcal{M}^{(t)}$$

其中稀疏掩码 $\mathcal{M}^{(t)}$ 在训练过程中逐步更新：
- 初期：允许更多权重参与更新
- 中期：逐步固定2:4模式
- 后期：仅更新非零权重

**知识蒸馏增强：**
使用稠密教师模型指导稀疏学生模型：
$$\mathcal{L}_{total} = \mathcal{L}_{task} + \lambda \mathcal{L}_{KD}(f_{sparse}, f_{dense})$$

### 10.1.5 与其他稀疏模式的对比

| 稀疏模式 | 稀疏率 | 硬件支持 | 精度保持 | 实际加速比 |
|---------|--------|----------|----------|------------|
| 2:4结构化 | 50% | 专用硬件 | 优秀 | 1.3-1.7x |
| 1:4结构化 | 75% | 理论可行 | 较差 | - |
| 块稀疏(8×8) | 灵活 | 部分支持 | 良好 | 1.2-1.5x |
| 非结构化 | 任意 | 困难 | 最佳 | <1x(通常) |

### 10.1.6 实际部署考虑

**编译器支持：**

2:4稀疏需要深度学习编译器的特殊支持。NVIDIA的cuSPARSELt库提供了完整的API：

```
稀疏化流程：
1. 权重分析：识别可稀疏化的层
2. 模式生成：基于幅值的2:4模式选择
3. 元数据编码：生成3比特位置索引
4. 内核选择：调用稀疏GEMM内核
```

**精度-性能权衡曲线：**

实验表明，2:4稀疏在不同模型上的表现：
- BERT：精度损失<0.5%，加速1.5x
- ResNet-50：精度损失<1%，加速1.4x  
- GPT类模型：需要更精细的层级配置

**层级敏感度分析：**

不同层对2:4稀疏的敏感度差异很大：

1. **嵌入层**：通常不适合稀疏化，信息密度高
2. **前馈网络层**：最适合2:4稀疏，冗余度高
3. **注意力投影层**：Q、K矩阵可稀疏，V矩阵需谨慎
4. **归一化层**：参数量小，稀疏化收益有限

### 10.1.7 高级优化技术

**动态稀疏重分配：**

在训练过程中动态调整稀疏模式：

$$\mathcal{M}^{(t+1)} = \text{Top-K}(|W^{(t)}| + \gamma \cdot |\nabla W^{(t)}|)$$

其中γ是梯度重要性权重，允许小权重但梯度大的参数被保留。

**结构化剪枝与蒸馏的协同：**

三阶段训练策略：
1. **预稀疏化**：使用L1正则化促进权重稀疏
2. **硬稀疏化**：应用2:4约束，固定模式
3. **蒸馏恢复**：使用原始稠密模型作为教师

损失函数设计：
$$\mathcal{L} = \mathcal{L}_{CE} + \alpha \cdot \mathcal{L}_{KL}(S||T) + \beta \cdot ||W||_1$$

### 10.1.8 边缘部署优化

**内存访问模式优化：**

2:4稀疏的内存访问具有规律性，可以优化：
- 预取策略：每次加载4个权重，使用2个
- 缓存对齐：确保4元素组不跨缓存行
- 向量化：使用SIMD指令处理多个2:4组

**功耗优化：**

稀疏计算的功耗节省来自于：
1. 减少的内存带宽：50%的数据传输
2. 跳过的MAC操作：50%的乘累加
3. 简化的控制逻辑：固定的访问模式

实测功耗降低：20-35%（取决于层类型和批大小）

### 10.1.9 2:4稀疏的未来发展

**硬件演进趋势：**

下一代硬件可能支持更灵活的结构化稀疏：
- 可配置N:M模式（运行时可调）
- 混合精度稀疏（INT8 + 2:4）
- 层级自适应稀疏

**算法创新方向：**

1. **学习型稀疏模式**：使用强化学习自动发现最优稀疏配置
2. **任务相关稀疏**：根据输入动态调整稀疏模式
3. **渐进式稀疏**：推理时从密集逐步过渡到稀疏

## 10.2 N:M稀疏模式设计

### 10.2.1 通用N:M稀疏框架

N:M稀疏是2:4模式的推广，表示每M个权重中保留N个非零值。稀疏率为 $s = 1 - N/M$。

**设计空间探索：**
- **细粒度**：M较小（如4, 8），硬件实现简单，但灵活性受限
- **粗粒度**：M较大（如16, 32），更灵活但硬件复杂度增加
- **混合粒度**：不同层使用不同的N:M配置

### 10.2.2 最优N:M配置选择

选择N:M配置需要平衡多个因素：

**信息论视角：**
每M个位置选N个的组合数：$C(M,N) = \binom{M}{N}$

索引开销（比特）：$\lceil \log_2 C(M,N) \rceil$

**硬件效率分析：**
- 索引解码复杂度：O(log M)
- 路由网络复杂度：O(M × N)
- 缓存行对齐：M应为缓存行大小的因子

**经验法则：**
1. M = 2^k（如4, 8, 16）便于硬件实现
2. N/M ≈ 0.5 在精度和压缩间取得平衡
3. 考虑目标硬件的SIMD宽度

### 10.2.3 自适应N:M稀疏

不同层对稀疏的敏感度不同，可以采用自适应策略：

**层级敏感度分析：**
使用泰勒展开估计剪枝影响：
$$\Delta \mathcal{L} \approx \sum_i \frac{\partial \mathcal{L}}{\partial w_i} \Delta w_i + \frac{1}{2} \sum_{i,j} \frac{\partial^2 \mathcal{L}}{\partial w_i \partial w_j} \Delta w_i \Delta w_j$$

**混合精度稀疏分配：**
- 第一层/最后层：较低稀疏率（如2:4）
- 中间层：较高稀疏率（如1:4或2:8）
- 注意力层：保守稀疏（如3:4）

### 10.2.4 块结构化稀疏

将N:M模式扩展到二维块：

**块稀疏定义：**
将权重矩阵分割为 B×B 的块，每个块要么全零要么保留。

**优势：**
- 更好的缓存局部性
- 适合矩阵乘法的分块算法
- 可利用现有的稠密计算核

**实现细节：**
```
块大小选择：
- GPU：通常 16×16 或 32×32
- CPU：8×8 或 16×16
- 考虑向量指令宽度
```

### 10.2.5 多级N:M稀疏

**分层级稀疏设计：**

在不同粒度上应用N:M稀疏：

1. **细粒度级**：2:4在每4个元素
2. **中粒度级**：4:16在每16个元素  
3. **粗粒度级**：1:4在每4个块（16×16）

这种分层设计可以实现更高的压缩率：
$$总稀疏率 = 1 - \prod_{i=1}^{L} (N_i/M_i)$$

**动态粒度选择：**

根据层的特性选择合适的粒度：
- 卷积层：适合块稀疏，利用空间局部性
- 全连接层：适合细粒度N:M
- 注意力层：混合使用不同粒度

### 10.2.6 N:M稀疏的训练技巧

**渐进式增加稀疏度：**

从较小的稀疏率开始，逐步增加：

```
Epoch 1-10: 2:4 (50稀疏)
Epoch 11-20: 2:8 (75稀疏)  
Epoch 21-30: 1:8 (87.5稀疏)
```

渐进式策略的数学描述：
$$N(t) = N_{final} + (N_{init} - N_{final}) \cdot e^{-\lambda t}$$

**稀疏模式的正则化：**

为了促进N:M模式的形成，可以设计特殊的正则项：

$$\mathcal{R}_{N:M} = \sum_{g \in \text{groups}} \max(0, ||g||_0 - N)^2$$

这个正则项惩罚超过N个非零元素的组。

**混合精度N:M稀疏：**

结合量化和稀疏化：
1. 先应用N:M稀疏
2. 对非零值进行INT8/INT4量化
3. 使用特殊的编码格式存储

总压缩率：
$$\text{Compression} = \frac{N}{M} \times \frac{\text{bits}_{quant}}{32}$$

### 10.2.7 硬件支持和加速

**通用N:M加速器设计：**

一个理想的N:M稀疏加速器应包含：

1. **索引解码单元**：快速解析N:M位置信息
2. **数据路由网络**：将非零值路由到正确的计算单元
3. **跳零逻辑**：避免零值计算
4. **缓冲管理**：高效的稀疏数据缓存

**软件模拟和优化：**

在没有专用硬件的情况下，可以通过软件优化：

```
CPU优化：
- SIMD指令集（AVX-512）
- 预取和缓存优化
- OpenMP并行化

GPU优化：  
- Warp级别的协作
- 共享内存的使用
- 动态并行度调整
```

### 10.2.8 实际应用案例

**vLLM中的N:M稀疏支持：**

vLLM框架在处理大语言模型时对N:M稀疏的处理：
- 自动检测硬件能力
- 根据模型大小选择合适的N:M配置
- 与PagedAttention结合优化内存使用

**SGLang的稀疏推理优化：**

SGLang在编译时进行稀疏模式分析：
- 静态分析模型的稀疏结构
- 生成优化的执行计划
- 融合稀疏操作和其他算子

### 10.2.9 N:M稀疏的未来方向

**可学习的N:M配置：**

使用神经架构搜索（NAS）来自动发现最优的N:M配置：
- 搜索空间：每层的(N,M)对
- 优化目标：精度、延迟、能耗的多目标优化
- 约束条件：硬件支持的模式

**动态N:M稀疏：**

根据输入内容动态调整稀疏模式：
$$N(x) = f_{\theta}(\text{features}(x))$$

其中$f_{\theta}$是一个轻量级网络，预测每层的最佳N值。

## 10.3 共享参数与模型合并

### 10.3.1 模型合并的数学基础

模型合并通过组合多个独立训练的模型来创建一个多功能模型。

**权重空间的几何视角：**
神经网络的损失景观存在多个局部最优点，这些点可能通过低损失路径相连。

**线性模式连接性（LMC）：**
给定两个模型 θ_1 和 θ_2，探索连接路径：
$$θ(α) = (1-α)θ_1 + αθ_2, \quad α \in [0,1]$$

如果 $\mathcal{L}(θ(α))$ 在整个区间保持较低值，说明存在线性连接性。

### 10.3.2 任务算术（Task Arithmetic）

任务算术将模型能力向量化，实现灵活的能力组合。

**任务向量定义：**
$$\tau_i = θ_{task_i} - θ_{base}$$

其中：
- $θ_{base}$：预训练基础模型
- $θ_{task_i}$：在任务i上微调后的模型
- $\tau_i$：任务i的"技能向量"

**算术操作：**
1. **能力添加**：$θ_{new} = θ_{base} + \lambda(\tau_A + \tau_B)$
2. **能力增强**：$θ_{new} = θ_{base} + \lambda \cdot \tau_A$（λ > 1）
3. **能力消除**：$θ_{new} = θ_{base} - \tau_A$

### 10.3.3 高级合并算法

**TIES-Merging算法：**

1. **修剪（Trim）**：
   保留每个任务向量中变化最大的k%参数：
   $$\tilde{\tau}_i = \tau_i \odot \mathbb{1}[|\tau_i| > \text{Percentile}(|\tau_i|, 100-k)]$$

2. **选举（Elect）**：
   解决符号冲突：
   $$\text{sign}_{elected} = \text{sign}\left(\sum_i \text{sign}(\tilde{\tau}_{i,j})\right)$$

3. **解耦合并（Disjoint Merge）**：
   只合并符号一致的参数：
   $$\theta_{merged,j} = θ_{base,j} + \frac{1}{|\mathcal{A}_j|} \sum_{i \in \mathcal{A}_j} \tilde{\tau}_{i,j}$$
   
   其中 $\mathcal{A}_j$ 是在位置j上符号与选举结果一致的任务集合。

**DARE（Drop And REscale）：**

引入随机性提高泛化：
1. **随机丢弃**：$\tilde{\tau} = \tau \odot \text{Bernoulli}(1-p)$
2. **重新缩放**：$\tilde{\tau} = \tilde{\tau} / (1-p)$

这保持了期望值不变：$\mathbb{E}[\tilde{\tau}] = \tau$

### 10.3.4 边缘场景的模型合并

**存储优化策略：**

1. **基础模型 + 任务向量**：
   - 存储一个通用基础模型（如LLaMA）
   - 各任务只存储小型任务向量
   - 存储减少：~90%（任务向量通常很稀疏）

2. **动态合并**：
   ```
   推理时动态组合：
   if task == "translation":
       θ = θ_base + τ_translation
   elif task == "summarization":
       θ = θ_base + τ_summarization
   ```

**延迟优化：**
- 预计算常用任务组合
- 缓存合并后的权重
- 使用量化的任务向量（INT8/INT4）

### 10.3.5 参数共享的理论基础

**神经网络的对称性：**

神经网络存在大量的对称性，这为参数共享提供了理论基础：

1. **置换对称性**：隐层神经元可以任意交换
2. **缩放对称性**：权重可以通过缩放保持功能不变
3. **旋转对称性**：特定架构下的权重旋转

**模式崩塌现象：**

在训练后期，模型权重往往会呈现某种模式：
$$W \approx \sum_{i=1}^{r} \sigma_i u_i v_i^T$$

其中$r$远小于矩阵的秩，这表明权重矩阵是低秩的。

### 10.3.6 高级模型合并技术

**模型汤（Model Soup）：**

通过平均多个独立训练的模型来提高泛化能力：

1. **均匀汤**：$\theta_{soup} = \frac{1}{K}\sum_{k=1}^{K} \theta_k$
2. **贪婪汤**：逐个添加模型，只保留提高性能的
3. **加权汤**：$\theta_{soup} = \sum_{k=1}^{K} w_k \theta_k$，其中$w_k$基于验证集性能

**模型缝合（Model Stitching）：**

将不同模型的部分组合起来：
- 前几层使用模型A（善于特征提取）
- 后几层使用模型B（善于任务特定推理）
- 需要额外的适配层

**参数共享的层级设计：**

1. **跨层共享**：
   ```
   Layer_i = Layer_j （完全共享）
   Layer_i = α*Layer_j + β*Layer_k （线性组合）
   ```

2. **循环层设计**：
   重复使用同一组参数：
   $$h_{t+1} = f(h_t, x; \theta_{shared})$$

### 10.3.7 模型合并的挑战与解决方案

**干扰问题：**

不同任务的更新可能相互干扰：
- 正干扰：两个任务在同一参数上有相反的更新
- 负干扰：一个任务的更新损害另一个任务的性能

**解决方案：**

1. **正交化任务向量**：
   $$\tau'_i = \tau_i - \sum_{j<i} \frac{\langle \tau_i, \tau_j \rangle}{||\tau_j||^2} \tau_j$$

2. **选择性合并**：
   只合并不冲突的参数更新

3. **多任务学习视角**：
   使用多任务学习的方法重新训练

### 10.3.8 实际应用案例分析

**Mistral的MoE架构：**

Mistral 8x7B使用了参数共享的思想：
- 8个专家共享基础参数
- 每个专家有独特的FFN层
- 根据输入动态选择2个专家

**LoRA合并实践：**

在实际使用中，LoRA合并常见的做法：
1. 训练多个LoRA适配器
2. 使用SVD分解合并后的权重
3. 重新量化以减小存储

**边缘设备上的实现：**

```
手机端部署：
- 基础模型：1GB（INT4量化）
- 任务向量：每个10-50MB
- 内存要求：<2GB
- 切换延迟：<100ms
```

### 10.3.9 未来发展趋势

**持续学习中的模型合并：**

随着模型不断更新，如何高效合并新旧版本：
- 增量式合并：只存储版本间的差异
- 动态权重分配：根据时间衰减旧版本权重
- 知识图谱引导：保留重要知识，更新过时内容

**跨模态模型合并：**

将视觉、语言、音频模型合并：
- 共享表征空间
- 跨模态对齐
- 统一的任务向量表示

## 10.4 稀疏张量的高效存储

### 10.4.1 稀疏存储格式

**COO（Coordinate）格式：**
存储三个数组：
- 行索引：row_indices[]
- 列索引：col_indices[]
- 非零值：values[]

存储开销：$O(nnz \times (2 \times \text{sizeof}(int) + \text{sizeof}(float)))$

**CSR（Compressed Sparse Row）格式：**
- 行指针：row_ptr[]（长度m+1）
- 列索引：col_indices[]（长度nnz）
- 非零值：values[]（长度nnz）

优势：行访问高效，适合矩阵-向量乘法

**块稀疏格式：**
将矩阵分块，只存储非零块：
- 块索引：block_indices[]
- 块数据：block_values[]（每个块密集存储）

### 10.4.2 硬件友好的稀疏格式

**2:4格式编码：**
```
原始：[0.1, 0, 0, 0.3] 
编码：
- 值：[0.1, 0.3]
- 索引：0b1001（二进制表示位置）
```

**向量化友好格式：**
对齐到SIMD宽度（如AVX-512的512位）：
- 将多个2:4组打包到一个向量寄存器
- 使用掩码寄存器进行选择性加载/存储

### 10.4.3 压缩技术集成

**量化 + 稀疏：**
1. 先进行2:4稀疏化
2. 对非零值进行INT8/INT4量化
3. 总压缩率：50%（稀疏）× 25%（INT8）= 12.5%

**聚类 + 稀疏：**
- 对非零权重进行K-means聚类
- 存储聚类中心和索引
- 适合权重分布有明显模式的情况

### 10.4.4 动态稀疏管理

**稀疏模式缓存：**
```
缓存结构：
- 模式ID -> 稀疏掩码
- LRU替换策略
- 预计算常见模式
```

**增量更新：**
对于在线学习场景，支持稀疏模式的增量更新：
$$\mathcal{M}^{(t+1)} = \alpha \mathcal{M}^{(t)} + (1-α) \mathcal{M}_{new}$$

## 本章小结

本章深入探讨了稀疏化和参数共享两大模型压缩技术：

1. **2:4结构化稀疏**：通过硬件-算法协同设计，实现了50%压缩率和1.3-1.7倍实际加速。其成功关键在于规则的稀疏模式使硬件能够高效跳过零计算。

2. **N:M稀疏设计**：提供了灵活的稀疏配置空间，需要在硬件复杂度、压缩率和精度间权衡。自适应和混合粒度策略可以进一步优化性能。

3. **模型合并技术**：从简单的权重平均到复杂的任务算术，提供了低成本的多任务模型构建方案。TIES-Merging和DARE等方法解决了参数冲突问题。

4. **稀疏存储优化**：不同的稀疏格式适用于不同场景，硬件友好的格式设计是实现加速的关键。量化与稀疏的结合可以达到极高的压缩率。

关键公式回顾：
- 2:4稀疏约束：$||G_i||_0 = 2$ for each 4-element group
- 任务向量：$\tau = θ_{task} - θ_{base}$
- TIES符号选举：$\text{sign}_{elected} = \text{sign}(\sum_i \text{sign}(\tau_i))$
- 存储效率：压缩率 = 稀疏率 × 量化压缩率

## 练习题

### 基础题

1. **2:4稀疏计算**
   给定权重向量 [0.5, -0.2, 0.8, -0.1, 0.3, 0.7, -0.6, 0.4]，应用2:4稀疏化。计算稀疏化后的向量和所需的索引位数。
   
   *Hint: 每4个元素为一组，保留绝对值最大的2个。*

2. **稀疏存储分析**
   一个 1024×1024 的矩阵采用2:4稀疏，使用FP16存储。计算采用COO格式和2:4专用格式的存储需求（字节）。
   
   *Hint: 2:4格式每4个元素需要2个FP16值和3比特索引。*

3. **任务向量计算**
   基础模型参数 θ_base = [1.0, 2.0, 3.0]，两个任务微调后的参数分别为 θ_A = [1.2, 2.1, 2.8] 和 θ_B = [0.9, 2.3, 3.1]。计算任务向量并执行任务算术 θ_base + 0.5(τ_A + τ_B)。
   
   *Hint: 先计算每个任务向量，然后进行线性组合。*

4. **N:M配置选择**
   对于一个要求75%稀疏率的场景，列出所有可能的N:M配置（M≤8），并计算各自的索引开销。
   
   *Hint: 稀疏率 = 1 - N/M = 0.75*

### 挑战题

5. **混合稀疏优化**
   设计一个神经网络的层级稀疏方案：网络有10层，第1层和第10层对精度最敏感，中间层可以承受更高稀疏。在总体稀疏率达到60%的约束下，如何分配各层的N:M配置？
   
   *Hint: 考虑使用拉格朗日乘数法优化分配。*

6. **TIES-Merging分析**
   三个任务向量在某参数位置的更新分别为：τ_1 = +0.5, τ_2 = -0.3, τ_3 = +0.2。使用TIES-Merging的选举和合并策略，计算最终的参数更新。如果先应用50%的修剪阈值会如何影响结果？
   
   *Hint: 考虑符号投票和选择性平均。*

7. **稀疏模式的信息论分析**
   证明2:4稀疏模式的索引信息熵上界为log₂(6)比特。推广到一般的N:M稀疏，导出索引熵的公式。这对硬件设计有什么启示？
   
   *Hint: 使用组合数学和信息论基础。*

8. **动态稀疏调度**
   设计一个算法，在推理时根据输入的特征动态选择不同层的稀疏模式。目标是在保持精度的同时最大化吞吐量。考虑：如何快速评估每层的重要性？如何避免模式切换的开销？
   
   *Hint: 可以使用输入相关的敏感度度量，如梯度范数或激活值统计。*

<details>
<summary>答案（点击展开）</summary>

1. 稀疏化结果：[0.5, 0, 0.8, 0], [0, 0.7, -0.6, 0]。每组需要3比特索引，总共6比特。

2. COO格式：3×524,288×2 = 3,145,728字节。2:4格式：2×262,144×2 + 262,144×3/8 = 1,147,192字节。

3. τ_A = [0.2, 0.1, -0.2], τ_B = [-0.1, 0.3, 0.1]。结果：[1.05, 2.2, 2.95]。

4. 1:4（索引2比特）、2:8（索引约6.6比特）、3:12等，具体计算略。

5. 示例方案：第1、10层用3:4（25%稀疏），第2-9层用1:3（66.7%稀疏），总体约60%。

6. 符号投票：2正1负，选正。合并：(0.5+0.2)/2=0.35。修剪后可能只保留τ_1。

7. C(4,2)=6种组合，熵上界log₂(6)≈2.58比特。一般公式：H ≤ log₂(C(M,N))。

8. 算法框架：计算各层激活范数→映射到稀疏等级→缓存常用模式→批量切换。

</details>